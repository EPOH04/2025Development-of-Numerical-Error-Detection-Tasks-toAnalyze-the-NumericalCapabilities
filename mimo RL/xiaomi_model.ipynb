{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "_cell_guid": "b1076dfc-b9ad-4769-8c92-a6c4dae69d19",
    "_uuid": "8f2839f25d086af736a60e9eeb907d3b93b6e0e5",
    "execution": {
     "iopub.execute_input": "2025-05-28T03:58:54.275157Z",
     "iopub.status.busy": "2025-05-28T03:58:54.274856Z",
     "iopub.status.idle": "2025-05-28T03:58:58.013391Z",
     "shell.execute_reply": "2025-05-28T03:58:58.012908Z",
     "shell.execute_reply.started": "2025-05-28T03:58:54.275137Z"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Looking in indexes: https://mirrors.aliyun.com/pypi/simple/\n",
      "Requirement already satisfied: jsonlines in /usr/local/lib/python3.11/site-packages (4.0.0)\n",
      "Requirement already satisfied: attrs>=19.2.0 in /usr/local/lib/python3.11/site-packages (from jsonlines) (25.3.0)\n",
      "\u001b[33mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv\u001b[0m\u001b[33m\n",
      "\u001b[0m\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m A new release of pip is available: \u001b[0m\u001b[31;49m23.3.2\u001b[0m\u001b[39;49m -> \u001b[0m\u001b[32;49m25.1.1\u001b[0m\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m To update, run: \u001b[0m\u001b[32;49mpip install --upgrade pip\u001b[0m\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "pip install jsonlines"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "ExecutionIndicator": {
     "show": true
    },
    "execution": {
     "iopub.execute_input": "2025-06-04T08:54:24.616081Z",
     "iopub.status.busy": "2025-06-04T08:54:24.615823Z",
     "iopub.status.idle": "2025-06-04T08:55:45.200640Z",
     "shell.execute_reply": "2025-06-04T08:55:45.200208Z",
     "shell.execute_reply.started": "2025-06-04T08:54:24.616064Z"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.11/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "PyTorch version: 2.5.1+cu124\n",
      "CUDA available: True\n",
      "CUDA version: 12.4\n",
      "GPU: NVIDIA A10\n",
      "Total GPU memory: 21.98 GB\n",
      "Using device: cuda:0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loading checkpoint shards: 100%|██████████| 4/4 [01:15<00:00, 18.95s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model loaded successfully\n",
      "Model device: cuda:0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "import json\n",
    "import os\n",
    "from typing import List, Dict, Tuple\n",
    "import random\n",
    "from transformers import AutoTokenizer, AutoModelForCausalLM\n",
    "import torch\n",
    "from tqdm import tqdm\n",
    "import re\n",
    "from collections import defaultdict, Counter\n",
    "import time\n",
    "import logging\n",
    "import jsonlines\n",
    "\n",
    "# 检查环境和 GPU 信息\n",
    "print(f\"PyTorch version: {torch.__version__}\")\n",
    "print(f\"CUDA available: {torch.cuda.is_available()}\")\n",
    "if torch.cuda.is_available():\n",
    "    print(f\"CUDA version: {torch.version.cuda}\")\n",
    "    print(f\"GPU: {torch.cuda.get_device_name(0)}\")\n",
    "    print(f\"Total GPU memory: {torch.cuda.get_device_properties(0).total_memory / 1024**3:.2f} GB\")\n",
    "\n",
    "# 设置设备\n",
    "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(f\"Using device: {device}\")\n",
    "\n",
    "# 1. 加载 GLM-4-9B 模型和分词器\n",
    "model_name = \"/mnt/workspace/dataroot/models/mimo/mimoRL\"  # 确认 GLM-4-9B 模型名称\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name, trust_remote_code=True,local_files_only=True)\n",
    "tokenizer.padding_side = 'left'  # 确保与 Qwen2 一致\n",
    "model = AutoModelForCausalLM.from_pretrained(\n",
    "    model_name,\n",
    "    torch_dtype=torch.float16,  # 使用 float16 减少显存\n",
    "    device_map=\"cuda:0\",  # 强制加载到 P100\n",
    "    trust_remote_code=True,  # GLM-4-9B 可能需要\n",
    "    local_files_only=True\n",
    ")\n",
    "model.eval()\n",
    "print(\"Model loaded successfully\")\n",
    "print(f\"Model device: {next(model.parameters()).device}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "ExecutionIndicator": {
     "show": false
    },
    "execution": {
     "iopub.execute_input": "2025-06-02T11:03:35.758363Z",
     "iopub.status.busy": "2025-06-02T11:03:35.757949Z",
     "iopub.status.idle": "2025-06-02T11:03:35.777078Z",
     "shell.execute_reply": "2025-06-02T11:03:35.776641Z",
     "shell.execute_reply.started": "2025-06-02T11:03:35.758337Z"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "GPU memory allocated: 14.59 GB\n",
      "GPU memory reserved: 14.93 GB\n"
     ]
    }
   ],
   "source": [
    "# 打印初始显存\n",
    "if torch.cuda.is_available():\n",
    "    print(f\"GPU memory allocated: {torch.cuda.memory_allocated(0) / 1024**3:.2f} GB\")\n",
    "    print(f\"GPU memory reserved: {torch.cuda.memory_reserved(0) / 1024**3:.2f} GB\")\n",
    "\n",
    "# 2. 构造提示函数\n",
    "def create_zero_shot_prompt(passage: str, number: str) -> str:\n",
    "    return f\"\"\"### ROLE. You are an expert fact-checker specializing in numerical accuracy across biology, physics, history, mathematics, and everyday scenarios. ### TASK. Determine if the given number contains a factual error within the provided context. ### ANALYSIS PROCESS. Follow this reasoning sequence: 1. CONTEXT ANALYSIS: Identify the domain and type of measurement being described 2. GENERATED KNOWLEDGE: Recall established facts, typical ranges, and known standards for this specific domain and measurement type 3. PLAUSIBILITY CHECK: Compare the number against expected ranges, physical laws, biological constraints, historical accuracy, and mathematical consistency 4. ERROR DETECTION: Check for biological impossibilities, physical violations, historical inaccuracies, mathematical contradictions, or scale/magnitude errors. ### OUTPUT. Answer must only be \"Yes\" or \"No\",do not provide explanations. Yes = Contains factual error, No = Factually accurate. ### EVALUATION. Number to evaluate: \"{number}\",Passage: \"{passage}\". Is \"{number}\" in the following passage an error? \"{passage}\"\n",
    "Answer:\"\"\"\n",
    "\n",
    "def create_few_shot_prompt(passage: str, number: str) -> str:\n",
    "    examples = [\n",
    "        {\"passage\": \"Spiders have 9 limbs.\", \"number\": \"9\", \"answer\": \"Yes\"},\n",
    "        {\"passage\": \"Spiders have 8 limbs.\", \"number\": \"8\", \"answer\": \"No\"},\n",
    "        {\"passage\": \"Mike's height is -3.6 meters.\", \"number\": \"-3.6\", \"answer\": \"Yes\"},\n",
    "        {\"passage\": \"Mike's height is 1.8 meters.\", \"number\": \"1.8\", \"answer\": \"No\"}\n",
    "    ]\n",
    "    prompt = \"### ROLE. You are an expert fact-checker specializing in numerical accuracy across biology, physics, history, mathematics, and everyday scenarios. ### TASK. Determine if the given number contains a factual error within the provided context. ### ANALYSIS PROCESS. Follow this reasoning sequence: 1. CONTEXT ANALYSIS: Identify the domain and type of measurement being described 2. GENERATED KNOWLEDGE: Recall established facts, typical ranges, and known standards for this specific domain and measurement type 3. PLAUSIBILITY CHECK: Compare the number against expected ranges, physical laws, biological constraints, historical accuracy, and mathematical consistency 4. ERROR DETECTION: Check for biological impossibilities, physical violations, historical inaccuracies, mathematical contradictions, or scale/magnitude errors. ### OUTPUT. Answer must only be 'Yes' or 'No',do not provide explanations. Yes = Contains factual error, No = Factually accurate.\"\n",
    "    for ex in examples:\n",
    "        prompt += f\"\"\"Question: Is \"{ex['number']}\" in the following passage an error? \"{ex['passage']}\"\n",
    "Answer: {ex['answer']}\\n\"\"\"\n",
    "    prompt += f\"\"\"Question: Is \"{number}\" in the following passage an error? \"{passage}\"\n",
    "Answer:\"\"\"\n",
    "    return prompt\n",
    "\n",
    "# 3. 加载 BeNEDect 数据集\n",
    "def load_benedect_dataset(file_path: str) -> List[Dict]:\n",
    "    if not os.path.exists(file_path):\n",
    "        raise FileNotFoundError(f\"数据集文件 {file_path} 不存在，请确认路径！\")\n",
    "    try:\n",
    "        with open(file_path, 'r', encoding='utf-8') as f:\n",
    "            dataset_dict = json.load(f)\n",
    "    except json.JSONDecodeError as e:\n",
    "        raise ValueError(f\"JSON 文件解析错误：{e}\")\n",
    "    \n",
    "    dataset = list(dataset_dict.values())\n",
    "    dataset = dataset[:len(dataset) // 2]#!!!\n",
    "    save_list = []\n",
    "    \n",
    "    for i, data in enumerate(tqdm(dataset, desc=\"Processing dataset\")):\n",
    "        required_fields = ['correct_number', 'correct_passage', 'error_number', 'error_passage', 'dataset', 'operation']\n",
    "        for field in required_fields:\n",
    "            if field not in data:\n",
    "                print(f\"样本 {data.get('id', '未知')} 缺少字段 {field}\")\n",
    "                continue\n",
    "        \n",
    "        prompt_fn = create_few_shot_prompt if i % 48 == 0 else create_zero_shot_prompt\n",
    "        correct_item = {\n",
    "            \"prompt\": prompt_fn(data['correct_passage'], data['correct_number']),\n",
    "            \"expected_answer\": \"No\",\n",
    "            \"dataset\": data['dataset'],\n",
    "            \"operation\": data['operation'],\n",
    "            \"error_annotation\": data.get('error_annotation', {}),\n",
    "            \"passage\": data['correct_passage'],\n",
    "            \"number\": data['correct_number'],\n",
    "            \"prompt_type\": \"few_shot\" if i % 48 == 0 else \"zero_shot\"\n",
    "        }\n",
    "        error_item = {\n",
    "            \"prompt\": prompt_fn(data['error_passage'], data['error_number']),\n",
    "            \"expected_answer\": \"Yes\",\n",
    "            \"dataset\": data['dataset'],\n",
    "            \"operation\": data['operation'],\n",
    "            \"error_annotation\": data.get('error_annotation', {}),\n",
    "            \"passage\": data['error_passage'],\n",
    "            \"number\": data['error_number'],\n",
    "            \"prompt_type\": \"few_shot\" if i % 48 == 0 else \"zero_shot\"\n",
    "        }\n",
    "        save_list.append(correct_item)\n",
    "        save_list.append(error_item)\n",
    "    \n",
    "    return save_list\n",
    "\n",
    "# 4. 单条推理\n",
    "def predict_single(prompt: str, max_retries: int = 3) -> str:\n",
    "    print(f\"Single prediction prompt: {prompt[:100]}...\")\n",
    "    attempt = 0\n",
    "    success = False\n",
    "    prediction = None\n",
    "    \n",
    "    while attempt < max_retries and not success:\n",
    "        try:\n",
    "            inputs = tokenizer(prompt, return_tensors=\"pt\", truncation=True, max_length=512)\n",
    "            inputs = {k: v.to(device) for k, v in inputs.items()}\n",
    "            print(f\"Input device: {inputs['input_ids'].device}\")\n",
    "            print(f\"Input shape: {inputs['input_ids'].shape}\")\n",
    "            \n",
    "            with torch.no_grad():\n",
    "                outputs = model.generate(\n",
    "                    **inputs,\n",
    "                    max_new_tokens=5,\n",
    "                    pad_token_id=tokenizer.pad_token_id,\n",
    "                    eos_token_id=tokenizer.eos_token_id,\n",
    "                    do_sample=False,\n",
    "                    top_k=1,\n",
    "                    top_p=0.0\n",
    "                )\n",
    "            \n",
    "            prediction = tokenizer.decode(outputs[0][inputs['input_ids'].shape[1]:], skip_special_tokens=True).strip()\n",
    "            success = True\n",
    "            print(f\"Single prediction success: Raw Prediction: {prediction}\")\n",
    "            \n",
    "        except RuntimeError as e:\n",
    "            print(f\"单条推理失败（尝试 {attempt + 1}/{max_retries}）：{e}\")\n",
    "            attempt += 1\n",
    "            torch.cuda.empty_cache()\n",
    "            time.sleep(1)\n",
    "            if attempt == max_retries:\n",
    "                print(\"单条推理失败，跳过\")\n",
    "                prediction = \"generation_error\"\n",
    "        \n",
    "        finally:\n",
    "            if 'inputs' in locals():\n",
    "                for v in inputs.values():\n",
    "                    del v\n",
    "            torch.cuda.empty_cache()\n",
    "            print(f\"GPU memory after single prediction: {torch.cuda.memory_allocated(0) / 1024**3:.2f} GB\")\n",
    "    \n",
    "    return prediction\n",
    "\n",
    "# 5. 批次推理\n",
    "def predict_batch(prompts: List[str], batch_size: int = 8, max_retries: int = 3) -> List[str]:\n",
    "    predictions = []\n",
    "    \n",
    "    for i in tqdm(range(0, len(prompts), batch_size), desc=\"Predicting\"):\n",
    "        batch_prompts = prompts[i:i + batch_size]\n",
    "        attempt = 0\n",
    "        success = False\n",
    "        batch_preds = None\n",
    "        \n",
    "        while attempt < max_retries and not success:\n",
    "            try:\n",
    "                # 强制统一序列长度，检查张量形状\n",
    "                inputs = tokenizer(\n",
    "                    batch_prompts,\n",
    "                    return_tensors=\"pt\",\n",
    "                    padding=True,\n",
    "                    truncation=True,\n",
    "                    max_length=512,\n",
    "                    return_attention_mask=True\n",
    "                )\n",
    "                inputs = {k: v.to(device) for k, v in inputs.items()}\n",
    "                print(f\"Batch {i//batch_size} input shapes: input_ids={inputs['input_ids'].shape}, attention_mask={inputs['attention_mask'].shape}\")\n",
    "                \n",
    "                with torch.no_grad():\n",
    "                    outputs = model.generate(\n",
    "                        **inputs,\n",
    "                        max_new_tokens=5,\n",
    "                        pad_token_id=tokenizer.pad_token_id,\n",
    "                        eos_token_id=tokenizer.eos_token_id,\n",
    "                        do_sample=False,\n",
    "                        top_k=1,\n",
    "                        top_p=0.0\n",
    "                    )\n",
    "                \n",
    "                batch_preds = [\n",
    "                    tokenizer.decode(output[inputs['input_ids'].shape[1]:], skip_special_tokens=True).strip()\n",
    "                    for output in outputs\n",
    "                ]\n",
    "                success = True\n",
    "                if i % (10 * batch_size) == 0:\n",
    "                    print(f\"批次 {i//batch_size}: 原始预测: {batch_preds}\")\n",
    "                \n",
    "            except RuntimeError as e:\n",
    "                print(f\"批次推理失败（尝试 {attempt + 1}/{max_retries}）：{e}\")\n",
    "                attempt += 1\n",
    "                torch.cuda.empty_cache()\n",
    "                time.sleep(1)\n",
    "                if attempt == max_retries:\n",
    "                    print(f\"批次 {i//batch_size} 推理失败，跳过\")\n",
    "                    batch_preds = [\"generation_error\"] * len(batch_prompts)\n",
    "            \n",
    "            finally:\n",
    "                if 'inputs' in locals():\n",
    "                    for v in inputs.values():\n",
    "                        del v\n",
    "                torch.cuda.empty_cache()\n",
    "                print(f\"GPU memory after batch {i//batch_size}: {torch.cuda.memory_allocated(0) / 1024**3:.2f} GB\")\n",
    "        \n",
    "        predictions.extend(batch_preds)\n",
    "    \n",
    "    return predictions\n",
    "\n",
    "# 6. 保存预测结果到 JSONL\n",
    "def save_predictions_to_jsonl(data: List[Dict], predictions: List[str], output_file: str):\n",
    "    with jsonlines.open(output_file, mode='w') as writer:\n",
    "        for item, pred in zip(data, predictions):\n",
    "            result = {\n",
    "                \"prompt\": item['prompt'],\n",
    "                \"passage\": item['passage'],\n",
    "                \"number\": item['number'],\n",
    "                \"expected_answer\": item['expected_answer'],\n",
    "                \"raw_prediction\": pred,\n",
    "                \"dataset\": item['dataset'],\n",
    "                \"operation\": item['operation'],\n",
    "                \"error_annotation\": item['error_annotation'],\n",
    "                \"prompt_type\": item['prompt_type']\n",
    "            }\n",
    "            writer.write(result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-06-02T11:15:17.020380Z",
     "iopub.status.busy": "2025-06-02T11:15:17.020082Z",
     "iopub.status.idle": "2025-06-02T11:15:17.037731Z",
     "shell.execute_reply": "2025-06-02T11:15:17.037339Z",
     "shell.execute_reply.started": "2025-06-02T11:15:17.020364Z"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "GPU memory allocated: 14.60 GB\n",
      "GPU memory reserved: 14.85 GB\n"
     ]
    }
   ],
   "source": [
    "#original\n",
    "# 打印初始显存\n",
    "if torch.cuda.is_available():\n",
    "    print(f\"GPU memory allocated: {torch.cuda.memory_allocated(0) / 1024**3:.2f} GB\")\n",
    "    print(f\"GPU memory reserved: {torch.cuda.memory_reserved(0) / 1024**3:.2f} GB\")\n",
    "\n",
    "# 2. 构造提示函数\n",
    "def create_zero_shot_prompt(passage: str, number: str) -> str:\n",
    "    return f\"\"\"Answer with only 'Yes' or 'No'. Do not provide explanations. Is \"{number}\" in the following passage an error? \"{passage}\"\n",
    "Answer:\"\"\"\n",
    "\n",
    "def create_few_shot_prompt(passage: str, number: str) -> str:\n",
    "    examples = [\n",
    "        {\"passage\": \"Spiders have 9 limbs.\", \"number\": \"9\", \"answer\": \"Yes\"},\n",
    "        {\"passage\": \"Spiders have 8 limbs.\", \"number\": \"8\", \"answer\": \"No\"},\n",
    "        {\"passage\": \"Mike's height is -3.6 meters.\", \"number\": \"-3.6\", \"answer\": \"Yes\"},\n",
    "        {\"passage\": \"Mike's height is 1.8 meters.\", \"number\": \"1.8\", \"answer\": \"No\"}\n",
    "    ]\n",
    "    prompt = \"Answer with only 'Yes' or 'No'. Do not provide explanations.\\n\"\n",
    "    for ex in examples:\n",
    "        prompt += f\"\"\"Question: Is \"{ex['number']}\" in the following passage an error? \"{ex['passage']}\"\n",
    "Answer: {ex['answer']}\\n\"\"\"\n",
    "    prompt += f\"\"\"Question: Is \"{number}\" in the following passage an error? \"{passage}\"\n",
    "Answer:\"\"\"\n",
    "    return prompt\n",
    "\n",
    "# 3. 加载 BeNEDect 数据集\n",
    "def load_benedect_dataset(file_path: str) -> List[Dict]:\n",
    "    if not os.path.exists(file_path):\n",
    "        raise FileNotFoundError(f\"数据集文件 {file_path} 不存在，请确认路径！\")\n",
    "    try:\n",
    "        with open(file_path, 'r', encoding='utf-8') as f:\n",
    "            dataset_dict = json.load(f)\n",
    "    except json.JSONDecodeError as e:\n",
    "        raise ValueError(f\"JSON 文件解析错误：{e}\")\n",
    "    \n",
    "    dataset = list(dataset_dict.values())\n",
    "    save_list = []\n",
    "    \n",
    "    for i, data in enumerate(tqdm(dataset, desc=\"Processing dataset\")):\n",
    "        required_fields = ['correct_number', 'correct_passage', 'error_number', 'error_passage', 'dataset', 'operation']\n",
    "        for field in required_fields:\n",
    "            if field not in data:\n",
    "                print(f\"样本 {data.get('id', '未知')} 缺少字段 {field}\")\n",
    "                continue\n",
    "        \n",
    "        prompt_fn = create_few_shot_prompt if i % 48 == 0 else create_zero_shot_prompt\n",
    "        correct_item = {\n",
    "            \"prompt\": prompt_fn(data['correct_passage'], data['correct_number']),\n",
    "            \"expected_answer\": \"No\",\n",
    "            \"dataset\": data['dataset'],\n",
    "            \"operation\": data['operation'],\n",
    "            \"error_annotation\": data.get('error_annotation', {}),\n",
    "            \"passage\": data['correct_passage'],\n",
    "            \"number\": data['correct_number'],\n",
    "            \"prompt_type\": \"few_shot\" if i % 48 == 0 else \"zero_shot\"\n",
    "        }\n",
    "        error_item = {\n",
    "            \"prompt\": prompt_fn(data['error_passage'], data['error_number']),\n",
    "            \"expected_answer\": \"Yes\",\n",
    "            \"dataset\": data['dataset'],\n",
    "            \"operation\": data['operation'],\n",
    "            \"error_annotation\": data.get('error_annotation', {}),\n",
    "            \"passage\": data['error_passage'],\n",
    "            \"number\": data['error_number'],\n",
    "            \"prompt_type\": \"few_shot\" if i % 48 == 0 else \"zero_shot\"\n",
    "        }\n",
    "        save_list.append(correct_item)\n",
    "        save_list.append(error_item)\n",
    "    \n",
    "    return save_list\n",
    "\n",
    "# 4. 单条推理\n",
    "def predict_single(prompt: str, max_retries: int = 3) -> str:\n",
    "    print(f\"Single prediction prompt: {prompt[:100]}...\")\n",
    "    attempt = 0\n",
    "    success = False\n",
    "    prediction = None\n",
    "    \n",
    "    while attempt < max_retries and not success:\n",
    "        try:\n",
    "            inputs = tokenizer(prompt, return_tensors=\"pt\", truncation=True, max_length=512)\n",
    "            inputs = {k: v.to(device) for k, v in inputs.items()}\n",
    "            print(f\"Input device: {inputs['input_ids'].device}\")\n",
    "            print(f\"Input shape: {inputs['input_ids'].shape}\")\n",
    "            \n",
    "            with torch.no_grad():\n",
    "                outputs = model.generate(\n",
    "                    **inputs,\n",
    "                    max_new_tokens=5,\n",
    "                    pad_token_id=tokenizer.pad_token_id,\n",
    "                    eos_token_id=tokenizer.eos_token_id,\n",
    "                    do_sample=False,\n",
    "                    top_k=1,\n",
    "                    top_p=0.0\n",
    "                )\n",
    "            \n",
    "            prediction = tokenizer.decode(outputs[0][inputs['input_ids'].shape[1]:], skip_special_tokens=True).strip()\n",
    "            success = True\n",
    "            print(f\"Single prediction success: Raw Prediction: {prediction}\")\n",
    "            \n",
    "        except RuntimeError as e:\n",
    "            print(f\"单条推理失败（尝试 {attempt + 1}/{max_retries}）：{e}\")\n",
    "            attempt += 1\n",
    "            torch.cuda.empty_cache()\n",
    "            time.sleep(1)\n",
    "            if attempt == max_retries:\n",
    "                print(\"单条推理失败，跳过\")\n",
    "                prediction = \"generation_error\"\n",
    "        \n",
    "        finally:\n",
    "            if 'inputs' in locals():\n",
    "                for v in inputs.values():\n",
    "                    del v\n",
    "            torch.cuda.empty_cache()\n",
    "            print(f\"GPU memory after single prediction: {torch.cuda.memory_allocated(0) / 1024**3:.2f} GB\")\n",
    "    \n",
    "    return prediction\n",
    "\n",
    "# 5. 批次推理\n",
    "def predict_batch(prompts: List[str], batch_size: int = 8, max_retries: int = 3) -> List[str]:\n",
    "    predictions = []\n",
    "    \n",
    "    for i in tqdm(range(0, len(prompts), batch_size), desc=\"Predicting\"):\n",
    "        batch_prompts = prompts[i:i + batch_size]\n",
    "        attempt = 0\n",
    "        success = False\n",
    "        batch_preds = None\n",
    "        \n",
    "        while attempt < max_retries and not success:\n",
    "            try:\n",
    "                # 强制统一序列长度，检查张量形状\n",
    "                inputs = tokenizer(\n",
    "                    batch_prompts,\n",
    "                    return_tensors=\"pt\",\n",
    "                    padding=True,\n",
    "                    truncation=True,\n",
    "                    max_length=512,\n",
    "                    return_attention_mask=True\n",
    "                )\n",
    "                inputs = {k: v.to(device) for k, v in inputs.items()}\n",
    "                print(f\"Batch {i//batch_size} input shapes: input_ids={inputs['input_ids'].shape}, attention_mask={inputs['attention_mask'].shape}\")\n",
    "                \n",
    "                with torch.no_grad():\n",
    "                    outputs = model.generate(\n",
    "                        **inputs,\n",
    "                        max_new_tokens=5,\n",
    "                        pad_token_id=tokenizer.pad_token_id,\n",
    "                        eos_token_id=tokenizer.eos_token_id,\n",
    "                        do_sample=False,\n",
    "                        top_k=1,\n",
    "                        top_p=0.0\n",
    "                    )\n",
    "                \n",
    "                batch_preds = [\n",
    "                    tokenizer.decode(output[inputs['input_ids'].shape[1]:], skip_special_tokens=True).strip()\n",
    "                    for output in outputs\n",
    "                ]\n",
    "                success = True\n",
    "                if i % (10 * batch_size) == 0:\n",
    "                    print(f\"批次 {i//batch_size}: 原始预测: {batch_preds}\")\n",
    "                \n",
    "            except RuntimeError as e:\n",
    "                print(f\"批次推理失败（尝试 {attempt + 1}/{max_retries}）：{e}\")\n",
    "                attempt += 1\n",
    "                torch.cuda.empty_cache()\n",
    "                time.sleep(1)\n",
    "                if attempt == max_retries:\n",
    "                    print(f\"批次 {i//batch_size} 推理失败，跳过\")\n",
    "                    batch_preds = [\"generation_error\"] * len(batch_prompts)\n",
    "            \n",
    "            finally:\n",
    "                if 'inputs' in locals():\n",
    "                    for v in inputs.values():\n",
    "                        del v\n",
    "                torch.cuda.empty_cache()\n",
    "                print(f\"GPU memory after batch {i//batch_size}: {torch.cuda.memory_allocated(0) / 1024**3:.2f} GB\")\n",
    "        \n",
    "        predictions.extend(batch_preds)\n",
    "    \n",
    "    return predictions\n",
    "\n",
    "# 6. 保存预测结果到 JSONL\n",
    "def save_predictions_to_jsonl(data: List[Dict], predictions: List[str], output_file: str):\n",
    "    with jsonlines.open(output_file, mode='w') as writer:\n",
    "        for item, pred in zip(data, predictions):\n",
    "            result = {\n",
    "                \"prompt\": item['prompt'],\n",
    "                \"passage\": item['passage'],\n",
    "                \"number\": item['number'],\n",
    "                \"expected_answer\": item['expected_answer'],\n",
    "                \"raw_prediction\": pred,\n",
    "                \"dataset\": item['dataset'],\n",
    "                \"operation\": item['operation'],\n",
    "                \"error_annotation\": item['error_annotation'],\n",
    "                \"prompt_type\": item['prompt_type']\n",
    "            }\n",
    "            writer.write(result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "ExecutionIndicator": {
     "show": true
    },
    "execution": {
     "iopub.execute_input": "2025-06-04T08:55:45.201740Z",
     "iopub.status.busy": "2025-06-04T08:55:45.201466Z",
     "iopub.status.idle": "2025-06-04T08:55:45.221015Z",
     "shell.execute_reply": "2025-06-04T08:55:45.220625Z",
     "shell.execute_reply.started": "2025-06-04T08:55:45.201725Z"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "GPU memory allocated: 14.59 GB\n",
      "GPU memory reserved: 14.93 GB\n"
     ]
    }
   ],
   "source": [
    "#deepseek edit\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "if torch.cuda.is_available():\n",
    "    print(f\"GPU memory allocated: {torch.cuda.memory_allocated(0) / 1024**3:.2f} GB\")\n",
    "    print(f\"GPU memory reserved: {torch.cuda.memory_reserved(0) / 1024**3:.2f} GB\")\n",
    "\n",
    "    \n",
    "\n",
    "\n",
    "def create_zero_shot_prompt(passage: str, number: str) -> str:\n",
    "    return f\"\"\"### ROLE. You are an expert fact-checker specializing in numerical accuracy across biology, physics, history, mathematics, and everyday scenarios. ### TASK. Determine if the given number contains a factual error within the provided context. ### ANALYSIS PROCESS. Follow this reasoning sequence: 1. CONTEXT ANALYSIS: Identify the domain and type of measurement being described 2. GENERATED KNOWLEDGE: Recall established facts, typical ranges, and known standards for this specific domain and measurement type 3. PLAUSIBILITY CHECK: Compare the number against expected ranges, physical laws, biological constraints, historical accuracy, and mathematical consistency 4. ERROR DETECTION: Check for biological impossibilities, physical violations, historical inaccuracies, mathematical contradictions, or scale/magnitude errors. ### OUTPUT FORMAT. Answer must only be \"yes\" or \"no\". Do not provide any any explanations. ### EVALUATION. Is \"{number}\" in the following passage an error? \"{passage}\"?\n",
    "Answer:\"\"\"\n",
    "\n",
    "def create_few_shot_prompt(passage: str, number: str) -> str:\n",
    "    examples = [\n",
    "        {\"passage\": \"Spiders have 9 limbs.\", \"number\": \"9\", \"answer\": \"Yes\"},\n",
    "        {\"passage\": \"Spiders have 8 limbs.\", \"number\": \"8\", \"answer\": \"No\"},\n",
    "        {\"passage\": \"Mike's height is -3.6 meters.\", \"number\": \"-3.6\", \"answer\": \"Yes\"},\n",
    "        {\"passage\": \"Mike's height is 1.8 meters.\", \"number\": \"1.8\", \"answer\": \"No\"}\n",
    "    ]\n",
    "    prompt = f\"\"\"### ROLE. You are an expert fact-checker specializing in numerical accuracy across biology, physics, history, mathematics, and everyday scenarios. ### TASK. Determine if the given number contains a factual error within the provided context. ### ANALYSIS PROCESS. Follow this reasoning sequence: 1. CONTEXT ANALYSIS: Identify the domain and type of measurement being described 2. GENERATED KNOWLEDGE: Recall established facts, typical ranges, and known standards for this specific domain and measurement type 3. PLAUSIBILITY CHECK: Compare the number against expected ranges, physical laws, biological constraints, historical accuracy, and mathematical consistency 4. ERROR DETECTION: Check for biological impossibilities, physical violations, historical inaccuracies, mathematical contradictions, or scale/magnitude errors. ### OUTPUT FORMAT. Answer must only be \"Yes\" or \"No\". Do not provide any explanations. ### EVALUATION. Is \"{number}\" in the following passage an error? \"{passage}\"?\n",
    "Answer:\"\"\"\n",
    "    for ex in examples:\n",
    "        prompt += f\"\"\"Question: Is \"{ex['number']}\" in the following passage an error? \"{ex['passage']}\"\n",
    "Answer: {ex['answer']}\\n\"\"\"\n",
    "    prompt += f\"\"\"Question: Is \"{number}\" in the following passage an error? \"{passage}\"\n",
    "Answer:\"\"\"\n",
    "    return prompt\n",
    "\n",
    "# 3. 加载 BeNEDect 数据集\n",
    "def load_benedect_dataset(file_path: str) -> List[Dict]:\n",
    "    if not os.path.exists(file_path):\n",
    "        raise FileNotFoundError(f\"数据集文件 {file_path} 不存在，请确认路径！\")\n",
    "    try:\n",
    "        with open(file_path, 'r', encoding='utf-8') as f:\n",
    "            dataset_dict = json.load(f)\n",
    "    except json.JSONDecodeError as e:\n",
    "        raise ValueError(f\"JSON 文件解析错误：{e}\")\n",
    "    \n",
    "    dataset = list(dataset_dict.values())\n",
    "    #dataset = dataset[:len(dataset) // 2]#!!!\n",
    "    save_list = []\n",
    "    \n",
    "    for i, data in enumerate(tqdm(dataset, desc=\"Processing dataset\")):\n",
    "        required_fields = ['correct_number', 'correct_passage', 'error_number', 'error_passage', 'dataset', 'operation']\n",
    "        for field in required_fields:\n",
    "            if field not in data:\n",
    "                print(f\"样本 {data.get('id', '未知')} 缺少字段 {field}\")\n",
    "                continue\n",
    "        \n",
    "        prompt_fn = create_few_shot_prompt if i % 48 == 0 else create_zero_shot_prompt\n",
    "        correct_item = {\n",
    "            \"prompt\": prompt_fn(data['correct_passage'], data['correct_number']),\n",
    "            \"expected_answer\": \"No\",\n",
    "            \"dataset\": data['dataset'],\n",
    "            \"operation\": data['operation'],\n",
    "            \"error_annotation\": data.get('error_annotation', {}),\n",
    "            \"passage\": data['correct_passage'],\n",
    "            \"number\": data['correct_number'],\n",
    "            \"prompt_type\": \"few_shot\" if i % 48 == 0 else \"zero_shot\"\n",
    "        }\n",
    "        error_item = {\n",
    "            \"prompt\": prompt_fn(data['error_passage'], data['error_number']),\n",
    "            \"expected_answer\": \"Yes\",\n",
    "            \"dataset\": data['dataset'],\n",
    "            \"operation\": data['operation'],\n",
    "            \"error_annotation\": data.get('error_annotation', {}),\n",
    "            \"passage\": data['error_passage'],\n",
    "            \"number\": data['error_number'],\n",
    "            \"prompt_type\": \"few_shot\" if i % 48 == 0 else \"zero_shot\"\n",
    "        }\n",
    "        save_list.append(correct_item)\n",
    "        save_list.append(error_item)\n",
    "    \n",
    "    return save_list\n",
    "\n",
    "    \n",
    "def majority_vote(responses: List[str]) -> str:\n",
    "    \"\"\"Determine majority vote from 3 responses (Yes/No)\"\"\"\n",
    "    valid_responses = []\n",
    "    for r in responses:\n",
    "        r_low = r.lower().strip()\n",
    "        if r_low.startswith('y') or r_low == 'yes':\n",
    "            valid_responses.append(\"Yes\")\n",
    "        elif r_low.startswith('n') or r_low == 'no':\n",
    "            valid_responses.append(\"No\")\n",
    "    \n",
    "    if not valid_responses:\n",
    "        first = responses[0].lower().strip()\n",
    "        if first.startswith('y') or first == 'yes':\n",
    "            return \"Yes\"\n",
    "        else:\n",
    "            return \"No\"  # Default to No\n",
    "    \n",
    "    count_yes = valid_responses.count(\"Yes\")\n",
    "    count_no = valid_responses.count(\"No\")\n",
    "    \n",
    "    if count_yes > count_no:\n",
    "        return \"Yes\"\n",
    "    elif count_no > count_yes:\n",
    "        return \"No\"\n",
    "    else:\n",
    "        return valid_responses[0]  # Tie-breaker: first valid response\n",
    "\n",
    "def predict_single(prompt: str, max_retries: int = 3) -> str:\n",
    "    print(f\"Single prediction prompt: {prompt[:100]}...\")\n",
    "    attempt = 0\n",
    "    success = False\n",
    "    prediction = None\n",
    "    \n",
    "    while attempt < max_retries and not success:\n",
    "        try:\n",
    "            inputs = tokenizer(prompt, return_tensors=\"pt\", truncation=True, max_length=512)\n",
    "            inputs = {k: v.to(device) for k, v in inputs.items()}\n",
    "            \n",
    "            with torch.no_grad():\n",
    "                # Generate 3 sequences for voting\n",
    "                outputs = model.generate(\n",
    "                    **inputs,\n",
    "                    max_new_tokens=5,\n",
    "                    pad_token_id=tokenizer.pad_token_id,\n",
    "                    eos_token_id=tokenizer.eos_token_id,\n",
    "                    do_sample=True,\n",
    "                    temperature=0.3,\n",
    "                    top_p=0.5,\n",
    "                    num_return_sequences=3  # Key change: get 3 responses\n",
    "                )\n",
    "            \n",
    "            # Decode all 3 responses\n",
    "            responses = []\n",
    "            input_length = inputs['input_ids'].shape[1]\n",
    "            for i in range(3):\n",
    "                gen_tokens = outputs[i][input_length:]\n",
    "                responses.append(\n",
    "                    tokenizer.decode(gen_tokens, skip_special_tokens=True).strip()\n",
    "                )\n",
    "            \n",
    "            # Apply majority voting\n",
    "            prediction = majority_vote(responses)\n",
    "            success = True\n",
    "            \n",
    "        except RuntimeError as e:\n",
    "            print(f\"Retry {attempt+1}/{max_retries}: {e}\")\n",
    "            attempt += 1\n",
    "            torch.cuda.empty_cache()\n",
    "            time.sleep(1)\n",
    "            if attempt == max_retries:\n",
    "                prediction = \"generation_error\"\n",
    "        finally:\n",
    "            torch.cuda.empty_cache()\n",
    "    \n",
    "    return prediction\n",
    "\n",
    "\n",
    "def predict_batch(prompts: List[str], batch_size: int = 4, max_retries: int = 3) -> List[str]:\n",
    "    predictions = []\n",
    "    \n",
    "    for i in tqdm(range(0, len(prompts), batch_size), desc=\"Predicting\"):\n",
    "        batch_prompts = prompts[i:i + batch_size]\n",
    "        attempt = 0\n",
    "        success = False\n",
    "        batch_preds = [\"generation_error\"] * len(batch_prompts)  # Default\n",
    "        \n",
    "        while attempt < max_retries and not success:\n",
    "            try:\n",
    "                inputs = tokenizer(\n",
    "                    batch_prompts,\n",
    "                    return_tensors=\"pt\",\n",
    "                    padding=True,\n",
    "                    truncation=True,\n",
    "                    max_length=512\n",
    "                )\n",
    "                inputs = {k: v.to(device) for k, v in inputs.items()}\n",
    "                \n",
    "                with torch.no_grad():\n",
    "                    # Generate 3 sequences per prompt\n",
    "                    outputs = model.generate(\n",
    "                        **inputs,\n",
    "                        max_new_tokens=5,\n",
    "                        pad_token_id=tokenizer.pad_token_id,\n",
    "                        eos_token_id=tokenizer.eos_token_id,\n",
    "                        do_sample=True,\n",
    "                        temperature=0.3,\n",
    "                        top_p=0.5,\n",
    "                        num_return_sequences=3  # Key change\n",
    "                    )\n",
    "                \n",
    "                # Process responses (batch_size * 3 sequences)\n",
    "                input_length = inputs['input_ids'].shape[1]\n",
    "                batch_preds = []\n",
    "                for j in range(len(batch_prompts)):\n",
    "                    responses = []\n",
    "                    for k in range(3):\n",
    "                        idx = j * 3 + k\n",
    "                        gen_tokens = outputs[idx][input_length:]\n",
    "                        responses.append(\n",
    "                            tokenizer.decode(gen_tokens, skip_special_tokens=True).strip()\n",
    "                        )\n",
    "                    batch_preds.append(majority_vote(responses))\n",
    "                \n",
    "                success = True\n",
    "                \n",
    "            except RuntimeError as e:\n",
    "                print(f\"Batch {i}-{i+batch_size} retry {attempt+1}/{max_retries}: {e}\")\n",
    "                attempt += 1\n",
    "                torch.cuda.empty_cache()\n",
    "                time.sleep(2)\n",
    "            finally:\n",
    "                torch.cuda.empty_cache()\n",
    "        \n",
    "        predictions.extend(batch_preds)\n",
    "    \n",
    "    return predictions\n",
    "\n",
    "# 6. 保存预测结果到 JSONL\n",
    "def save_predictions_to_jsonl(data: List[Dict], predictions: List[str], output_file: str):\n",
    "    \"\"\"将原始预测结果保存到 JSONL 文件\"\"\"\n",
    "    with jsonlines.open(output_file, mode='w') as writer:\n",
    "        for item, pred in zip(data, predictions):\n",
    "            result = {\n",
    "                \"prompt\": item['prompt'],\n",
    "                \"passage\": item['passage'],\n",
    "                \"number\": item['number'],\n",
    "                \"expected_answer\": item['expected_answer'],\n",
    "                \"raw_prediction\": pred,  # 直接保存原始预测\n",
    "                \"dataset\": item['dataset'],\n",
    "                \"operation\": item['operation'],\n",
    "                \"error_annotation\": item['error_annotation'],\n",
    "                \"prompt_type\": item['prompt_type']\n",
    "            }\n",
    "            writer.write(result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "ExecutionIndicator": {
     "show": true
    },
    "execution": {
     "iopub.execute_input": "2025-06-04T08:55:45.221578Z",
     "iopub.status.busy": "2025-06-04T08:55:45.221446Z",
     "iopub.status.idle": "2025-06-04T09:46:15.176570Z",
     "shell.execute_reply": "2025-06-04T09:46:15.176016Z",
     "shell.execute_reply.started": "2025-06-04T08:55:45.221566Z"
    },
    "scrolled": true,
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing dataset: 100%|██████████| 4800/4800 [00:00<00:00, 286504.33it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=== 随机样本推理 ===\n",
      "Passage: The Tepehuán suffered a series of devastating epidemics of European-introduced diseases in the years before the revolt. Epidemics were known to have occurred in their region in 1594, 1601-1602, 1606-1607, 1610, and 1616-1617.  The Tepehuán and their neighbors may have been reduced in population by more than 80 percent by the epidemics, from a pre-Columbian population of more than 100,000 to fewer than 20,000,  of which the Tepehuán may have been one-half of this total During the Chichimeca war  the Tepehuán remained neutral although urged by the Chichimecas to join them in resistance to Spanish expansion. The Spanish failed to defeat the Chichimeca militarily and instituted a new policy called \"peace by purchase\" in which Catholic missionaries would be a major tool in pacifying hostile and semi-hostile Indians.  Indians were to be supplied with food and tools and resettled into towns. Missionaries, rather than the military, would take on most of the responsibility for integrating the Indians into Mexican and Christian society.  The Acaxee and Xixime were the first to have this new Spanish policy applied to them and the Tepehuán would be next.\n",
      "Number: 1602\n",
      "Expected Answer: No\n",
      "Prompt Type: zero_shot\n",
      "Prompt:\n",
      "### ROLE. You are an expert fact-checker specializing in numerical accuracy across biology, physics, history, mathematics, and everyday scenarios. ### TASK. Determine if the given number contains a factual error within the provided context. ### ANALYSIS PROCESS. Follow this reasoning sequence: 1. CONTEXT ANALYSIS: Identify the domain and type of measurement being described 2. GENERATED KNOWLEDGE: Recall established facts, typical ranges, and known standards for this specific domain and measurement type 3. PLAUSIBILITY CHECK: Compare the number against expected ranges, physical laws, biological constraints, historical accuracy, and mathematical consistency 4. ERROR DETECTION: Check for biological impossibilities, physical violations, historical inaccuracies, mathematical contradictions, or scale/magnitude errors. ### OUTPUT FORMAT. Answer must only be \"yes\" or \"no\". Do not provide any any explanations. ### EVALUATION. Is \"1602\" in the following passage an error? \"The Tepehuán suffered a series of devastating epidemics of European-introduced diseases in the years before the revolt. Epidemics were known to have occurred in their region in 1594, 1601-1602, 1606-1607, 1610, and 1616-1617.  The Tepehuán and their neighbors may have been reduced in population by more than 80 percent by the epidemics, from a pre-Columbian population of more than 100,000 to fewer than 20,000,  of which the Tepehuán may have been one-half of this total During the Chichimeca war  the Tepehuán remained neutral although urged by the Chichimecas to join them in resistance to Spanish expansion. The Spanish failed to defeat the Chichimeca militarily and instituted a new policy called \"peace by purchase\" in which Catholic missionaries would be a major tool in pacifying hostile and semi-hostile Indians.  Indians were to be supplied with food and tools and resettled into towns. Missionaries, rather than the military, would take on most of the responsibility for integrating the Indians into Mexican and Christian society.  The Acaxee and Xixime were the first to have this new Spanish policy applied to them and the Tepehuán would be next.\"?\n",
      "Answer:\n",
      "Single prediction prompt: ### ROLE. You are an expert fact-checker specializing in numerical accuracy across biology, physics,...\n",
      "Raw Prediction: Yes\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Predicting: 100%|██████████| 2400/2400 [50:28<00:00,  1.26s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "预测结果已保存到 /mnt/workspace/model_eval_first/mimo_7b/xiaomi_predictions.jsonl\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "file_path = \"/mnt/workspace/model_eval_first/BeNEDect_all.json\"  # 确认路径\n",
    "dataset = load_benedect_dataset(file_path)\n",
    "\n",
    "random_sample = random.choice(dataset)\n",
    "print(\"=== 随机样本推理 ===\")\n",
    "print(f\"Passage: {random_sample['passage']}\")\n",
    "print(f\"Number: {random_sample['number']}\")\n",
    "print(f\"Expected Answer: {random_sample['expected_answer']}\")\n",
    "print(f\"Prompt Type: {random_sample['prompt_type']}\")\n",
    "print(f\"Prompt:\\n{random_sample['prompt']}\")\n",
    "\n",
    "random_pred = predict_single(random_sample['prompt'])\n",
    "print(f\"Raw Prediction: {random_pred}\")\n",
    "\n",
    "prompts = [item['prompt'] for item in dataset]\n",
    "predictions = predict_batch(prompts, batch_size=4, max_retries=3)  # 减小批次大小\n",
    "output_file = \"/mnt/workspace/model_eval_first/mimo_7b/xiaomi_predictions.jsonl\"\n",
    "save_predictions_to_jsonl(dataset, predictions, output_file)\n",
    "print(f\"预测结果已保存到 {output_file}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-06-04T09:46:26.228621Z",
     "iopub.status.busy": "2025-06-04T09:46:26.228345Z",
     "iopub.status.idle": "2025-06-04T09:46:26.231172Z",
     "shell.execute_reply": "2025-06-04T09:46:26.230756Z",
     "shell.execute_reply.started": "2025-06-04T09:46:26.228605Z"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "import json\n",
    "from collections import Counter, defaultdict\n",
    "from typing import List, Dict, Tuple"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-06-04T09:46:27.293092Z",
     "iopub.status.busy": "2025-06-04T09:46:27.292663Z",
     "iopub.status.idle": "2025-06-04T09:46:27.302470Z",
     "shell.execute_reply": "2025-06-04T09:46:27.302093Z",
     "shell.execute_reply.started": "2025-06-04T09:46:27.293075Z"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "def parse_prediction(raw_prediction: str) -> str:\n",
    "    raw_prediction = raw_prediction.lower()\n",
    "    if 'yes' in raw_prediction:\n",
    "        return 'yes'\n",
    "    elif 'no' in raw_prediction:\n",
    "        return 'no'\n",
    "    else:\n",
    "        # print(f\"无法解析响应: {raw_prediction}\")\n",
    "        return 'generation_error'\n",
    "\n",
    "def evaluate_model(data_list: List[Dict], unparsed_output_file: str = 'unparsed_predictions.json') -> Tuple[Dict, Dict]:\n",
    "    metrics = Counter()\n",
    "    detailed_metrics = {\n",
    "        'by_domain': defaultdict(Counter),\n",
    "        'by_error_type': defaultdict(Counter),\n",
    "        'by_operation': defaultdict(Counter),\n",
    "        'by_prompt_type': defaultdict(Counter)\n",
    "    }\n",
    "    unparsed_data = {}  # 存储无法解析的样本，格式为 {id: {...}}\n",
    "    \n",
    "    for idx, item in enumerate(data_list):\n",
    "        expected = item['expected_answer'].lower()  # Yes/No 转为小写\n",
    "        pred = parse_prediction(item['raw_prediction'])\n",
    "        item['parsel_prediction'] = pred  # 保存解析结果\n",
    "        \n",
    "        # 如果无法解析，添加到 unparsed_data\n",
    "        if pred == 'generation_error':\n",
    "            # 只保存 expected_answer == \"Yes\" 的样本（错误样本）\n",
    "            if expected == 'yes':\n",
    "                sample_id = f\"unparsed_{idx}\"\n",
    "                unparsed_data[sample_id] = {\n",
    "                    \"error_number\": item['number'],\n",
    "                    \"error_passage\": item['passage'],\n",
    "                    \"dataset\": item['dataset'],\n",
    "                    \"operation\": item['operation'],\n",
    "                    \"error_annotation\": item['error_annotation'],\n",
    "                    # 以下字段需补充（若有正确数据）\n",
    "                    \"correct_number\": \"\",  # 需手动补充或从原始数据推导\n",
    "                    \"correct_passage\": \"\"  # 需手动补充或从原始数据推导\n",
    "                }\n",
    "        \n",
    "        domain = item['dataset']\n",
    "        operation = item['operation']\n",
    "        prompt_type = item['prompt_type']\n",
    "        error_types = [k for k, v in item['error_annotation'].items() if v > 0]\n",
    "        \n",
    "        # 计算总体指标\n",
    "        if pred == expected:\n",
    "            if expected == 'yes':\n",
    "                metrics['TP'] += 1\n",
    "                for et in error_types:\n",
    "                    detailed_metrics['by_error_type'][et]['TP'] += 1\n",
    "                detailed_metrics['by_domain'][domain]['TP'] += 1\n",
    "                detailed_metrics['by_operation'][operation]['TP'] += 1\n",
    "                detailed_metrics['by_prompt_type'][prompt_type]['TP'] += 1\n",
    "            else:  # expected == 'no'\n",
    "                metrics['TN'] += 1\n",
    "                for et in error_types:\n",
    "                    detailed_metrics['by_error_type'][et]['TN'] += 1\n",
    "                detailed_metrics['by_domain'][domain]['TN'] += 1\n",
    "                detailed_metrics['by_operation'][operation]['TN'] += 1\n",
    "                detailed_metrics['by_prompt_type'][prompt_type]['TN'] += 1\n",
    "        else:\n",
    "            if expected == 'yes':\n",
    "                metrics['FN'] += 1\n",
    "                for et in error_types:\n",
    "                    detailed_metrics['by_error_type'][et]['FN'] += 1\n",
    "                detailed_metrics['by_domain'][domain]['FN'] += 1\n",
    "                detailed_metrics['by_operation'][operation]['FN'] += 1\n",
    "                detailed_metrics['by_prompt_type'][prompt_type]['FN'] += 1\n",
    "            else:  # expected == 'no'\n",
    "                metrics['FP'] += 1\n",
    "                for et in error_types:\n",
    "                    detailed_metrics['by_error_type'][et]['FP'] += 1\n",
    "                detailed_metrics['by_domain'][domain]['FP'] += 1\n",
    "                detailed_metrics['by_operation'][operation]['FP'] += 1\n",
    "                detailed_metrics['by_prompt_type'][prompt_type]['FP'] += 1\n",
    "        \n",
    "        if pred == 'generation_error':\n",
    "            metrics['Generation Error'] += 1\n",
    "            for et in error_types:\n",
    "                detailed_metrics['by_error_type'][et]['Generation Error'] += 1\n",
    "            detailed_metrics['by_domain'][domain]['Generation Error'] += 1\n",
    "            detailed_metrics['by_operation'][operation]['Generation Error'] += 1\n",
    "            detailed_metrics['by_prompt_type'][prompt_type]['Generation Error'] += 1\n",
    "    \n",
    "    # 保存无法解析的数据到 JSON\n",
    "    if unparsed_data:\n",
    "        with open(unparsed_output_file, 'w', encoding='utf-8') as f:\n",
    "            json.dump(unparsed_data, f, indent=2, ensure_ascii=False)\n",
    "        # print(f\"无法解析的 {len(unparsed_data)} 条数据已保存到 {unparsed_output_file}\")\n",
    "        print(\"注意：JSON 文件仅包含 expected_answer='Yes' 的样本，correct_number 和 correct_passage 需手动补充\")\n",
    "    else:\n",
    "        print(\"没有无法解析的数据\")\n",
    "    \n",
    "    total = len(data_list)\n",
    "    metrics['Accuracy'] = (metrics['TP'] + metrics['TN']) / total if total > 0 else 0\n",
    "    return metrics, detailed_metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "ExecutionIndicator": {
     "show": true
    },
    "execution": {
     "iopub.execute_input": "2025-06-04T09:46:28.016020Z",
     "iopub.status.busy": "2025-06-04T09:46:28.015728Z",
     "iopub.status.idle": "2025-06-04T09:46:28.147643Z",
     "shell.execute_reply": "2025-06-04T09:46:28.147253Z",
     "shell.execute_reply.started": "2025-06-04T09:46:28.016002Z"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "没有无法解析的数据\n",
      "\n",
      "Overall Metrics:\n",
      "FP: 3284 (0.342)\n",
      "FN: 1479 (0.154)\n",
      "TP: 3321 (0.346)\n",
      "TN: 1516 (0.158)\n",
      "Accuracy: 0.504\n",
      "\n",
      "Metrics by Domain:\n",
      "Numeracy_600K_article_title: {'FP': 888, 'FN': 52, 'TP': 948, 'TN': 112}\n",
      "aclsent: {'FP': 884, 'TP': 891, 'TN': 64, 'FN': 57}\n",
      "DROP: {'FP': 401, 'TP': 396, 'TN': 593, 'FN': 598}\n",
      "qa-text-source-comparison: {'FP': 407, 'TP': 390, 'TN': 517, 'FN': 534}\n",
      "FinNum: {'FP': 704, 'TP': 696, 'TN': 230, 'FN': 238}\n",
      "\n",
      "Metrics by Error Type:\n",
      "Error in Number Relationships: {'FP': 137, 'FN': 64, 'TP': 132, 'TN': 59}\n",
      "Undetectable Error: {'FP': 348, 'TP': 343, 'FN': 121, 'TN': 116}\n",
      "Type Error: {'FP': 401, 'TP': 402, 'TN': 117, 'FN': 116}\n",
      "Anomaly: {'FP': 160, 'TP': 161, 'TN': 70, 'FN': 69}\n",
      "Improper Data: {'FP': 21, 'TP': 20, 'TN': 8, 'FN': 9}\n",
      "Factual Error: {'FP': 35, 'TP': 30, 'FN': 27, 'TN': 22}\n",
      "\n",
      "Metrics by Operation:\n",
      "*2: {'FP': 108, 'FN': 51, 'TP': 108, 'TN': 51}\n",
      "-10: {'FP': 125, 'TP': 132, 'TN': 58, 'FN': 51}\n",
      "+1: {'FP': 133, 'TP': 131, 'TN': 55, 'FN': 57}\n",
      "*0.9: {'FP': 118, 'TP': 123, 'TN': 68, 'FN': 63}\n",
      "*1.1: {'FP': 135, 'TP': 132, 'TN': 64, 'FN': 67}\n",
      "-0.5: {'FP': 111, 'TP': 103, 'TN': 45, 'FN': 53}\n",
      "+1000: {'TN': 51, 'TP': 109, 'FP': 108, 'FN': 50}\n",
      "*1.5: {'FP': 123, 'TP': 119, 'TN': 41, 'FN': 45}\n",
      "*0.1: {'FP': 115, 'TP': 110, 'FN': 51, 'TN': 46}\n",
      "*0: {'FP': 105, 'TP': 132, 'TN': 73, 'FN': 46}\n",
      "-0.1: {'FP': 132, 'TP': 125, 'TN': 56, 'FN': 63}\n",
      "swap: {'FP': 238, 'TP': 248, 'TN': 110, 'FN': 100}\n",
      "*0.01: {'FP': 133, 'TP': 137, 'TN': 48, 'FN': 44}\n",
      "+10: {'FP': 118, 'TP': 113, 'TN': 56, 'FN': 61}\n",
      "+0.1: {'FP': 105, 'TP': 108, 'TN': 58, 'FN': 55}\n",
      "*(-1): {'FP': 110, 'TP': 119, 'TN': 50, 'FN': 41}\n",
      "-1: {'FP': 128, 'FN': 71, 'TP': 129, 'TN': 72}\n",
      "*0.5: {'FP': 146, 'TP': 146, 'TN': 63, 'FN': 63}\n",
      "*100: {'FP': 105, 'TP': 109, 'FN': 52, 'TN': 56}\n",
      "+0.5: {'FP': 114, 'TP': 116, 'TN': 55, 'FN': 53}\n",
      "-1000: {'FP': 106, 'TP': 107, 'TN': 58, 'FN': 57}\n",
      "-100: {'FP': 109, 'TP': 110, 'TN': 46, 'FN': 45}\n",
      "*0.001: {'FP': 107, 'TP': 116, 'TN': 50, 'FN': 41}\n",
      "*0.7: {'FP': 95, 'TP': 85, 'TN': 34, 'FN': 44}\n",
      "*1000: {'FP': 126, 'TP': 126, 'TN': 45, 'FN': 45}\n",
      "+100: {'FP': 116, 'TP': 113, 'FN': 52, 'TN': 49}\n",
      "*10: {'FP': 115, 'TP': 115, 'TN': 58, 'FN': 58}\n",
      "\n",
      "Metrics by Prompt Type:\n",
      "few_shot: {'FP': 19, 'FN': 79, 'TN': 81, 'TP': 21}\n",
      "zero_shot: {'FP': 3265, 'TP': 3300, 'TN': 1435, 'FN': 1400}\n"
     ]
    }
   ],
   "source": [
    "# 读取 predictions.jsonl\n",
    "data_list = []\n",
    "input_file = 'xiaomi_predictions.jsonl'  # 确认路径\n",
    "with open(input_file, 'r', encoding='utf-8') as f:\n",
    "    for line in f:\n",
    "        data = json.loads(line)\n",
    "        data_list.append(data)\n",
    "\n",
    "# 评测模型并保存无法解析的数据\n",
    "unparsed_output_file = 'xiaomi_unparsed_predictions.json'\n",
    "metrics, detailed_metrics = evaluate_model(data_list, unparsed_output_file)\n",
    "\n",
    "# 打印总体指标\n",
    "print(\"\\nOverall Metrics:\")\n",
    "total = len(data_list)\n",
    "for key, value in metrics.items():\n",
    "    if key == 'Accuracy':\n",
    "        print(f\"{key}: {value:.3f}\")\n",
    "    else:\n",
    "        print(f\"{key}: {value} ({value / total:.3f})\")\n",
    "\n",
    "# 打印分维度指标\n",
    "print(\"\\nMetrics by Domain:\")\n",
    "for domain, counts in detailed_metrics['by_domain'].items():\n",
    "    print(f\"{domain}: {dict(counts)}\")\n",
    "\n",
    "print(\"\\nMetrics by Error Type:\")\n",
    "for error_type, counts in detailed_metrics['by_error_type'].items():\n",
    "    print(f\"{error_type}: {dict(counts)}\")\n",
    "\n",
    "print(\"\\nMetrics by Operation:\")\n",
    "for operation, counts in detailed_metrics['by_operation'].items():\n",
    "    print(f\"{operation}: {dict(counts)}\")\n",
    "\n",
    "print(\"\\nMetrics by Prompt Type:\")\n",
    "for prompt_type, counts in detailed_metrics['by_prompt_type'].items():\n",
    "    print(f\"{prompt_type}: {dict(counts)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kaggle": {
   "accelerator": "gpu",
   "dataSources": [
    {
     "datasetId": 7381062,
     "sourceId": 11757485,
     "sourceType": "datasetVersion"
    },
    {
     "datasetId": 7386290,
     "sourceId": 11765571,
     "sourceType": "datasetVersion"
    }
   ],
   "dockerImageVersionId": 31011,
   "isGpuEnabled": true,
   "isInternetEnabled": true,
   "language": "python",
   "sourceType": "notebook"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
