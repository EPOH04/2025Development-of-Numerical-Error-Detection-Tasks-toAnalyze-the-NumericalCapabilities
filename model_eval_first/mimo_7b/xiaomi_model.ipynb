{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "_cell_guid": "b1076dfc-b9ad-4769-8c92-a6c4dae69d19",
    "_uuid": "8f2839f25d086af736a60e9eeb907d3b93b6e0e5",
    "execution": {
     "iopub.execute_input": "2025-05-11T03:43:50.158264Z",
     "iopub.status.busy": "2025-05-11T03:43:50.158041Z",
     "iopub.status.idle": "2025-05-11T03:43:54.097175Z",
     "shell.execute_reply": "2025-05-11T03:43:54.096298Z",
     "shell.execute_reply.started": "2025-05-11T03:43:50.158241Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting jsonlines\n",
      "  Downloading jsonlines-4.0.0-py3-none-any.whl.metadata (1.6 kB)\n",
      "Requirement already satisfied: attrs>=19.2.0 in /usr/local/lib/python3.11/dist-packages (from jsonlines) (25.3.0)\n",
      "Downloading jsonlines-4.0.0-py3-none-any.whl (8.7 kB)\n",
      "Installing collected packages: jsonlines\n",
      "Successfully installed jsonlines-4.0.0\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "pip install jsonlines"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-05-11T03:43:56.062832Z",
     "iopub.status.busy": "2025-05-11T03:43:56.062106Z",
     "iopub.status.idle": "2025-05-11T03:48:14.713022Z",
     "shell.execute_reply": "2025-05-11T03:48:14.712188Z",
     "shell.execute_reply.started": "2025-05-11T03:43:56.062800Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "PyTorch version: 2.5.1+cu124\n",
      "CUDA available: True\n",
      "CUDA version: 12.4\n",
      "GPU: Tesla P100-PCIE-16GB\n",
      "Total GPU memory: 15.89 GB\n",
      "Using device: cuda:0\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "8a9f766db3dd4a8aa60344e5382c6bac",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer_config.json:   0%|          | 0.00/7.22k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "25b98c97a368427f8ddcd0a25018c3d6",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "vocab.json:   0%|          | 0.00/2.78M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e62faed1c8eb46ea94d2e8f0b5fd12d1",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "merges.txt:   0%|          | 0.00/1.67M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "4e9058b3fb404683916295fe3dd9a629",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer.json:   0%|          | 0.00/7.03M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a105e551c9af41168fd3647a718aef24",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "config.json:   0%|          | 0.00/988 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "9f676bf4216643918e2367f412c2b058",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "configuration_mimo.py:   0%|          | 0.00/376 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "A new version of the following files was downloaded from https://huggingface.co/XiaomiMiMo/MiMo-7B-RL:\n",
      "- configuration_mimo.py\n",
      ". Make sure to double-check they do not contain any added malicious code. To avoid downloading new versions of the code file, you can pin a revision.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "8e50aa31938e4c5d9d5ff7a2543c2a56",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "modeling_mimo.py:   0%|          | 0.00/3.53k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "A new version of the following files was downloaded from https://huggingface.co/XiaomiMiMo/MiMo-7B-RL:\n",
      "- modeling_mimo.py\n",
      ". Make sure to double-check they do not contain any added malicious code. To avoid downloading new versions of the code file, you can pin a revision.\n",
      "2025-05-11 03:44:17.221191: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:477] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n",
      "WARNING: All log messages before absl::InitializeLog() is called are written to STDERR\n",
      "E0000 00:00:1746935057.413141      31 cuda_dnn.cc:8310] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n",
      "E0000 00:00:1746935057.464688      31 cuda_blas.cc:1418] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e33537b00e1c472c9a9a118ef3f899a2",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model.safetensors.index.json:   0%|          | 0.00/37.0k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "bced44f1be6f4af083b2fca241b3ed07",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Fetching 4 files:   0%|          | 0/4 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "9b00da70878748ffb487efaebc30db2b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model-00003-of-00004.safetensors:   0%|          | 0.00/3.98G [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "95b84c75d92a4915bf55041745424604",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model-00001-of-00004.safetensors:   0%|          | 0.00/3.99G [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "85abf89121a24733a8304e300eac70b2",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model-00004-of-00004.safetensors:   0%|          | 0.00/3.71G [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "6e9d30d879c349e182468ae5e701f7c9",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model-00002-of-00004.safetensors:   0%|          | 0.00/3.99G [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Sliding Window Attention is enabled but not implemented for `sdpa`; unexpected results may be encountered.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "2bb18c6c210f4a89a0286d96c75f932a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/4 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ca7a511bf5bd4ce2a73cbc8fb4b6aa23",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "generation_config.json:   0%|          | 0.00/138 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model loaded successfully\n",
      "Model device: cuda:0\n"
     ]
    }
   ],
   "source": [
    "import json\n",
    "import os\n",
    "from typing import List, Dict, Tuple\n",
    "import random\n",
    "from transformers import AutoTokenizer, AutoModelForCausalLM\n",
    "import torch\n",
    "from tqdm import tqdm\n",
    "import re\n",
    "from collections import defaultdict, Counter\n",
    "import time\n",
    "import logging\n",
    "import jsonlines\n",
    "\n",
    "# 检查环境和 GPU 信息\n",
    "print(f\"PyTorch version: {torch.__version__}\")\n",
    "print(f\"CUDA available: {torch.cuda.is_available()}\")\n",
    "if torch.cuda.is_available():\n",
    "    print(f\"CUDA version: {torch.version.cuda}\")\n",
    "    print(f\"GPU: {torch.cuda.get_device_name(0)}\")\n",
    "    print(f\"Total GPU memory: {torch.cuda.get_device_properties(0).total_memory / 1024**3:.2f} GB\")\n",
    "\n",
    "# 设置设备\n",
    "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(f\"Using device: {device}\")\n",
    "\n",
    "# 1. 加载 GLM-4-9B 模型和分词器\n",
    "model_name = \"XiaomiMiMo/MiMo-7B-RL\"  # 确认 GLM-4-9B 模型名称\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name, trust_remote_code=True)\n",
    "tokenizer.padding_side = 'left'  # 确保与 Qwen2 一致\n",
    "model = AutoModelForCausalLM.from_pretrained(\n",
    "    model_name,\n",
    "    torch_dtype=torch.float16,  # 使用 float16 减少显存\n",
    "    device_map=\"cuda:0\",  # 强制加载到 P100\n",
    "    trust_remote_code=True  # GLM-4-9B 可能需要\n",
    ")\n",
    "model.eval()\n",
    "print(\"Model loaded successfully\")\n",
    "print(f\"Model device: {next(model.parameters()).device}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-05-11T03:48:20.291116Z",
     "iopub.status.busy": "2025-05-11T03:48:20.289881Z",
     "iopub.status.idle": "2025-05-11T03:48:20.313232Z",
     "shell.execute_reply": "2025-05-11T03:48:20.312412Z",
     "shell.execute_reply.started": "2025-05-11T03:48:20.291074Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "GPU memory allocated: 14.59 GB\n",
      "GPU memory reserved: 14.93 GB\n"
     ]
    }
   ],
   "source": [
    "# 打印初始显存\n",
    "if torch.cuda.is_available():\n",
    "    print(f\"GPU memory allocated: {torch.cuda.memory_allocated(0) / 1024**3:.2f} GB\")\n",
    "    print(f\"GPU memory reserved: {torch.cuda.memory_reserved(0) / 1024**3:.2f} GB\")\n",
    "\n",
    "# 2. 构造提示函数\n",
    "def create_zero_shot_prompt(passage: str, number: str) -> str:\n",
    "    return f\"\"\"Answer with only 'Yes' or 'No'. Do not provide explanations. Is \"{number}\" in the following passage an error? \"{passage}\"\n",
    "Answer:\"\"\"\n",
    "\n",
    "def create_few_shot_prompt(passage: str, number: str) -> str:\n",
    "    examples = [\n",
    "        {\"passage\": \"Spiders have 9 limbs.\", \"number\": \"9\", \"answer\": \"Yes\"},\n",
    "        {\"passage\": \"Spiders have 8 limbs.\", \"number\": \"8\", \"answer\": \"No\"},\n",
    "        {\"passage\": \"Mike's height is -3.6 meters.\", \"number\": \"-3.6\", \"answer\": \"Yes\"},\n",
    "        {\"passage\": \"Mike's height is 1.8 meters.\", \"number\": \"1.8\", \"answer\": \"No\"}\n",
    "    ]\n",
    "    prompt = \"Answer with only 'Yes' or 'No'. Do not provide explanations.\\n\"\n",
    "    for ex in examples:\n",
    "        prompt += f\"\"\"Question: Is \"{ex['number']}\" in the following passage an error? \"{ex['passage']}\"\n",
    "Answer: {ex['answer']}\\n\"\"\"\n",
    "    prompt += f\"\"\"Question: Is \"{number}\" in the following passage an error? \"{passage}\"\n",
    "Answer:\"\"\"\n",
    "    return prompt\n",
    "\n",
    "# 3. 加载 BeNEDect 数据集\n",
    "def load_benedect_dataset(file_path: str) -> List[Dict]:\n",
    "    if not os.path.exists(file_path):\n",
    "        raise FileNotFoundError(f\"数据集文件 {file_path} 不存在，请确认路径！\")\n",
    "    try:\n",
    "        with open(file_path, 'r', encoding='utf-8') as f:\n",
    "            dataset_dict = json.load(f)\n",
    "    except json.JSONDecodeError as e:\n",
    "        raise ValueError(f\"JSON 文件解析错误：{e}\")\n",
    "    \n",
    "    dataset = list(dataset_dict.values())\n",
    "    save_list = []\n",
    "    \n",
    "    for i, data in enumerate(tqdm(dataset, desc=\"Processing dataset\")):\n",
    "        required_fields = ['correct_number', 'correct_passage', 'error_number', 'error_passage', 'dataset', 'operation']\n",
    "        for field in required_fields:\n",
    "            if field not in data:\n",
    "                print(f\"样本 {data.get('id', '未知')} 缺少字段 {field}\")\n",
    "                continue\n",
    "        \n",
    "        prompt_fn = create_few_shot_prompt if i % 48 == 0 else create_zero_shot_prompt\n",
    "        correct_item = {\n",
    "            \"prompt\": prompt_fn(data['correct_passage'], data['correct_number']),\n",
    "            \"expected_answer\": \"No\",\n",
    "            \"dataset\": data['dataset'],\n",
    "            \"operation\": data['operation'],\n",
    "            \"error_annotation\": data.get('error_annotation', {}),\n",
    "            \"passage\": data['correct_passage'],\n",
    "            \"number\": data['correct_number'],\n",
    "            \"prompt_type\": \"few_shot\" if i % 48 == 0 else \"zero_shot\"\n",
    "        }\n",
    "        error_item = {\n",
    "            \"prompt\": prompt_fn(data['error_passage'], data['error_number']),\n",
    "            \"expected_answer\": \"Yes\",\n",
    "            \"dataset\": data['dataset'],\n",
    "            \"operation\": data['operation'],\n",
    "            \"error_annotation\": data.get('error_annotation', {}),\n",
    "            \"passage\": data['error_passage'],\n",
    "            \"number\": data['error_number'],\n",
    "            \"prompt_type\": \"few_shot\" if i % 48 == 0 else \"zero_shot\"\n",
    "        }\n",
    "        save_list.append(correct_item)\n",
    "        save_list.append(error_item)\n",
    "    \n",
    "    return save_list\n",
    "\n",
    "# 4. 单条推理\n",
    "def predict_single(prompt: str, max_retries: int = 3) -> str:\n",
    "    print(f\"Single prediction prompt: {prompt[:100]}...\")\n",
    "    attempt = 0\n",
    "    success = False\n",
    "    prediction = None\n",
    "    \n",
    "    while attempt < max_retries and not success:\n",
    "        try:\n",
    "            inputs = tokenizer(prompt, return_tensors=\"pt\", truncation=True, max_length=512)\n",
    "            inputs = {k: v.to(device) for k, v in inputs.items()}\n",
    "            print(f\"Input device: {inputs['input_ids'].device}\")\n",
    "            print(f\"Input shape: {inputs['input_ids'].shape}\")\n",
    "            \n",
    "            with torch.no_grad():\n",
    "                outputs = model.generate(\n",
    "                    **inputs,\n",
    "                    max_new_tokens=5,\n",
    "                    pad_token_id=tokenizer.pad_token_id,\n",
    "                    eos_token_id=tokenizer.eos_token_id,\n",
    "                    do_sample=False,\n",
    "                    top_k=1,\n",
    "                    top_p=0.0\n",
    "                )\n",
    "            \n",
    "            prediction = tokenizer.decode(outputs[0][inputs['input_ids'].shape[1]:], skip_special_tokens=True).strip()\n",
    "            success = True\n",
    "            print(f\"Single prediction success: Raw Prediction: {prediction}\")\n",
    "            \n",
    "        except RuntimeError as e:\n",
    "            print(f\"单条推理失败（尝试 {attempt + 1}/{max_retries}）：{e}\")\n",
    "            attempt += 1\n",
    "            torch.cuda.empty_cache()\n",
    "            time.sleep(1)\n",
    "            if attempt == max_retries:\n",
    "                print(\"单条推理失败，跳过\")\n",
    "                prediction = \"generation_error\"\n",
    "        \n",
    "        finally:\n",
    "            if 'inputs' in locals():\n",
    "                for v in inputs.values():\n",
    "                    del v\n",
    "            torch.cuda.empty_cache()\n",
    "            print(f\"GPU memory after single prediction: {torch.cuda.memory_allocated(0) / 1024**3:.2f} GB\")\n",
    "    \n",
    "    return prediction\n",
    "\n",
    "# 5. 批次推理\n",
    "def predict_batch(prompts: List[str], batch_size: int = 8, max_retries: int = 3) -> List[str]:\n",
    "    predictions = []\n",
    "    \n",
    "    for i in tqdm(range(0, len(prompts), batch_size), desc=\"Predicting\"):\n",
    "        batch_prompts = prompts[i:i + batch_size]\n",
    "        attempt = 0\n",
    "        success = False\n",
    "        batch_preds = None\n",
    "        \n",
    "        while attempt < max_retries and not success:\n",
    "            try:\n",
    "                # 强制统一序列长度，检查张量形状\n",
    "                inputs = tokenizer(\n",
    "                    batch_prompts,\n",
    "                    return_tensors=\"pt\",\n",
    "                    padding=True,\n",
    "                    truncation=True,\n",
    "                    max_length=512,\n",
    "                    return_attention_mask=True\n",
    "                )\n",
    "                inputs = {k: v.to(device) for k, v in inputs.items()}\n",
    "                print(f\"Batch {i//batch_size} input shapes: input_ids={inputs['input_ids'].shape}, attention_mask={inputs['attention_mask'].shape}\")\n",
    "                \n",
    "                with torch.no_grad():\n",
    "                    outputs = model.generate(\n",
    "                        **inputs,\n",
    "                        max_new_tokens=5,\n",
    "                        pad_token_id=tokenizer.pad_token_id,\n",
    "                        eos_token_id=tokenizer.eos_token_id,\n",
    "                        do_sample=False,\n",
    "                        top_k=1,\n",
    "                        top_p=0.0\n",
    "                    )\n",
    "                \n",
    "                batch_preds = [\n",
    "                    tokenizer.decode(output[inputs['input_ids'].shape[1]:], skip_special_tokens=True).strip()\n",
    "                    for output in outputs\n",
    "                ]\n",
    "                success = True\n",
    "                if i % (10 * batch_size) == 0:\n",
    "                    print(f\"批次 {i//batch_size}: 原始预测: {batch_preds}\")\n",
    "                \n",
    "            except RuntimeError as e:\n",
    "                print(f\"批次推理失败（尝试 {attempt + 1}/{max_retries}）：{e}\")\n",
    "                attempt += 1\n",
    "                torch.cuda.empty_cache()\n",
    "                time.sleep(1)\n",
    "                if attempt == max_retries:\n",
    "                    print(f\"批次 {i//batch_size} 推理失败，跳过\")\n",
    "                    batch_preds = [\"generation_error\"] * len(batch_prompts)\n",
    "            \n",
    "            finally:\n",
    "                if 'inputs' in locals():\n",
    "                    for v in inputs.values():\n",
    "                        del v\n",
    "                torch.cuda.empty_cache()\n",
    "                print(f\"GPU memory after batch {i//batch_size}: {torch.cuda.memory_allocated(0) / 1024**3:.2f} GB\")\n",
    "        \n",
    "        predictions.extend(batch_preds)\n",
    "    \n",
    "    return predictions\n",
    "\n",
    "# 6. 保存预测结果到 JSONL\n",
    "def save_predictions_to_jsonl(data: List[Dict], predictions: List[str], output_file: str):\n",
    "    with jsonlines.open(output_file, mode='w') as writer:\n",
    "        for item, pred in zip(data, predictions):\n",
    "            result = {\n",
    "                \"prompt\": item['prompt'],\n",
    "                \"passage\": item['passage'],\n",
    "                \"number\": item['number'],\n",
    "                \"expected_answer\": item['expected_answer'],\n",
    "                \"raw_prediction\": pred,\n",
    "                \"dataset\": item['dataset'],\n",
    "                \"operation\": item['operation'],\n",
    "                \"error_annotation\": item['error_annotation'],\n",
    "                \"prompt_type\": item['prompt_type']\n",
    "            }\n",
    "            writer.write(result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-05-11T03:48:25.081974Z",
     "iopub.status.busy": "2025-05-11T03:48:25.081711Z",
     "iopub.status.idle": "2025-05-11T03:58:54.869191Z",
     "shell.execute_reply": "2025-05-11T03:58:54.868494Z",
     "shell.execute_reply.started": "2025-05-11T03:48:25.081954Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing dataset: 100%|██████████| 327/327 [00:00<00:00, 89972.28it/s]\n",
      "/usr/local/lib/python3.11/dist-packages/transformers/generation/configuration_utils.py:636: UserWarning: `do_sample` is set to `False`. However, `top_p` is set to `0.0` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `top_p`.\n",
      "  warnings.warn(\n",
      "/usr/local/lib/python3.11/dist-packages/transformers/generation/configuration_utils.py:653: UserWarning: `do_sample` is set to `False`. However, `top_k` is set to `1` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `top_k`.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=== 随机样本推理 ===\n",
      "Passage: In a rematch of their only loss so far this season, the Patriots faced the Buffalo Bills in a road game. After taking the opening kick, the Bills raced 70 yards all the way to the Patriots 5, but the Patriots defense kept Buffalo out of the end zone, forcing them to kick a 23-yard field goal. On the ensuing kickoff, Walt Powell forced Matthew Slater to fumble, but Shea McClellin recovered for the Patriots at their own 30. The Patriots drained over half of the remainder of the first quarter, going 70 yards to score on a 9-yard touchdown pass from Brady to Amendola. After a Bills three-and-out, the Patriots were given good field at the Bills 45. An offensive pass interference penalty on Amendola moved the ball back to the Patriots 45, but a holding penalty on Robert Blanton of the Bills moved the ball to mid field. A two-yard run by Blount and incomplete pass brought up a 3rd-and-8 from the Bills 48, and Brady hit Edelman with a 47-yard bomb to the 1-yard line, but the gain was nullified on an ineligible man downfield penalty on Marcus Cannon moving the ball back to the Patriots 47. Then Brady hit Hogan on a 53-yard touchdown bomb, increasing the lead to 14-3. Four possessions later, the Bills raced 59 yards in just 5 plays, scoring on a Mike Gillislee 3-yard touchdown rush. After Amendola returned the ball 24 yards to the Patriots 21, the Patriots countered, using just four plays and Brady found Gronkowski on a 53-yard touchdown, increasing the lead to 21-10. The Bills raced 44 yards in 11 plays to the Patriots 31 on their ensuing possession, but Carpenter missed a 49-yard field goal after it hit the upright. Taking over at their own 39, a 6-yard pass to White, a 16-yard pass to Amendola, and a 7-yard pass to Edelman led the Patriot to Bills 33 and Gostkowski kicked a 51-yard field goal, giving the Patriots a 24-10 lead at halftime. Amendola kicked off the second half with fireworks, returning the kick 73 yards to the Bills 24. Two plays later, Brady hit Edelman for a 12-yard touchdown pass, extending the lead to 31-10. The Bills didn't quit, though, using a marathon 75-yard, six minute drive, concluding with Taylor splitting the defense for a 26-yard touchdown run, making it a game again with a 31-17 score. The Patriots countered right back, engineering an 11-play, 75-yard drive with LeGarrette Blount chalking up a 1-yard touchdown run, increasing the lead to 38-17 late in the third quarter. Using Reggie Bush 35 yard kick return, the Bills drove 50 yards to the Patriots 10 in just 4 plays, but on the first play of the fourth quarter, a Taylor pass bounced off the fingertips of Charles Clay and the Bills turned the ball over on downs. The Patriots put the game away on their next drive, marching 76-yards in 13 plays, taking 7:49 off the clock, with Gostkowski adding a 32-yard field goal, giving the Patriots a 41-17 lead with just 7:06 remaining in the game. The Bills drove 52 yards to the Patriots 32 on their next drive, but turned the ball over on downs when Robert Woods was tackled 1-yard short of the first down by Malcolm Butler. The Bills forced a Patriots punt and raced 66 yards in 6 plays, scoring a touchdown on a 1-yard touchdown run by Jonathan Williams with a two-point conversion on a two-yard pass from backup quarterback EJ Manuel to Nick O'Leary trimming the score to 41-25, but only 0:30 remained. After recovering the onside kick, the Patriots took a knee to win the game. Brady was 22/33 for 315 yards and had four touchdown passes to four different receivers (Danny Amendola, Chris Hogan, Rob Gronkowski and Julian Edelman); LeGarette Blount added a running touchdown and Stephen Gostkowski a field goal. Heading into their bye week, the Patriots improved to 7-1, the best record in the league.\n",
      "Number: 16\n",
      "Expected Answer: Yes\n",
      "Prompt Type: zero_shot\n",
      "Prompt:\n",
      "Answer with only 'Yes' or 'No'. Do not provide explanations. Is \"16\" in the following passage an error? \"In a rematch of their only loss so far this season, the Patriots faced the Buffalo Bills in a road game. After taking the opening kick, the Bills raced 70 yards all the way to the Patriots 5, but the Patriots defense kept Buffalo out of the end zone, forcing them to kick a 23-yard field goal. On the ensuing kickoff, Walt Powell forced Matthew Slater to fumble, but Shea McClellin recovered for the Patriots at their own 30. The Patriots drained over half of the remainder of the first quarter, going 70 yards to score on a 9-yard touchdown pass from Brady to Amendola. After a Bills three-and-out, the Patriots were given good field at the Bills 45. An offensive pass interference penalty on Amendola moved the ball back to the Patriots 45, but a holding penalty on Robert Blanton of the Bills moved the ball to mid field. A two-yard run by Blount and incomplete pass brought up a 3rd-and-8 from the Bills 48, and Brady hit Edelman with a 47-yard bomb to the 1-yard line, but the gain was nullified on an ineligible man downfield penalty on Marcus Cannon moving the ball back to the Patriots 47. Then Brady hit Hogan on a 53-yard touchdown bomb, increasing the lead to 14-3. Four possessions later, the Bills raced 59 yards in just 5 plays, scoring on a Mike Gillislee 3-yard touchdown rush. After Amendola returned the ball 24 yards to the Patriots 21, the Patriots countered, using just four plays and Brady found Gronkowski on a 53-yard touchdown, increasing the lead to 21-10. The Bills raced 44 yards in 11 plays to the Patriots 31 on their ensuing possession, but Carpenter missed a 49-yard field goal after it hit the upright. Taking over at their own 39, a 6-yard pass to White, a 16-yard pass to Amendola, and a 7-yard pass to Edelman led the Patriot to Bills 33 and Gostkowski kicked a 51-yard field goal, giving the Patriots a 24-10 lead at halftime. Amendola kicked off the second half with fireworks, returning the kick 73 yards to the Bills 24. Two plays later, Brady hit Edelman for a 12-yard touchdown pass, extending the lead to 31-10. The Bills didn't quit, though, using a marathon 75-yard, six minute drive, concluding with Taylor splitting the defense for a 26-yard touchdown run, making it a game again with a 31-17 score. The Patriots countered right back, engineering an 11-play, 75-yard drive with LeGarrette Blount chalking up a 1-yard touchdown run, increasing the lead to 38-17 late in the third quarter. Using Reggie Bush 35 yard kick return, the Bills drove 50 yards to the Patriots 10 in just 4 plays, but on the first play of the fourth quarter, a Taylor pass bounced off the fingertips of Charles Clay and the Bills turned the ball over on downs. The Patriots put the game away on their next drive, marching 76-yards in 13 plays, taking 7:49 off the clock, with Gostkowski adding a 32-yard field goal, giving the Patriots a 41-17 lead with just 7:06 remaining in the game. The Bills drove 52 yards to the Patriots 32 on their next drive, but turned the ball over on downs when Robert Woods was tackled 1-yard short of the first down by Malcolm Butler. The Bills forced a Patriots punt and raced 66 yards in 6 plays, scoring a touchdown on a 1-yard touchdown run by Jonathan Williams with a two-point conversion on a two-yard pass from backup quarterback EJ Manuel to Nick O'Leary trimming the score to 41-25, but only 0:30 remained. After recovering the onside kick, the Patriots took a knee to win the game. Brady was 22/33 for 315 yards and had four touchdown passes to four different receivers (Danny Amendola, Chris Hogan, Rob Gronkowski and Julian Edelman); LeGarette Blount added a running touchdown and Stephen Gostkowski a field goal. Heading into their bye week, the Patriots improved to 7-1, the best record in the league.\"\n",
      "Answer:\n",
      "Single prediction prompt: Answer with only 'Yes' or 'No'. Do not provide explanations. Is \"16\" in the following passage an err...\n",
      "Input device: cuda:0\n",
      "Input shape: torch.Size([1, 512])\n",
      "Single prediction success: Raw Prediction: returning the opening kick to\n",
      "GPU memory after single prediction: 14.60 GB\n",
      "Raw Prediction: returning the opening kick to\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Predicting:   0%|          | 0/164 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Batch 0 input shapes: input_ids=torch.Size([4, 175]), attention_mask=torch.Size([4, 175])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Predicting:   1%|          | 1/164 [00:02<05:41,  2.10s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "批次 0: 原始预测: ['No\\n\\nQuestion: Is', 'Yes\\n\\nQuestion: Is', 'Yes\\n\\nIs \"the', '<think>\\nOkay']\n",
      "GPU memory after batch 0: 14.60 GB\n",
      "Batch 1 input shapes: input_ids=torch.Size([4, 145]), attention_mask=torch.Size([4, 145])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Predicting:   1%|          | 2/164 [00:03<05:10,  1.92s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "GPU memory after batch 1: 14.60 GB\n",
      "Batch 2 input shapes: input_ids=torch.Size([4, 512]), attention_mask=torch.Size([4, 512])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Predicting:   2%|▏         | 3/164 [00:08<07:54,  2.95s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "GPU memory after batch 2: 14.60 GB\n",
      "Batch 3 input shapes: input_ids=torch.Size([4, 512]), attention_mask=torch.Size([4, 512])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Predicting:   2%|▏         | 4/164 [00:12<09:08,  3.43s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "GPU memory after batch 3: 14.60 GB\n",
      "Batch 4 input shapes: input_ids=torch.Size([4, 512]), attention_mask=torch.Size([4, 512])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Predicting:   3%|▎         | 5/164 [00:16<09:47,  3.70s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "GPU memory after batch 4: 14.60 GB\n",
      "Batch 5 input shapes: input_ids=torch.Size([4, 364]), attention_mask=torch.Size([4, 364])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Predicting:   4%|▎         | 6/164 [00:20<09:39,  3.67s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "GPU memory after batch 5: 14.60 GB\n",
      "Batch 6 input shapes: input_ids=torch.Size([4, 512]), attention_mask=torch.Size([4, 512])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Predicting:   4%|▍         | 7/164 [00:24<10:01,  3.83s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "GPU memory after batch 6: 14.60 GB\n",
      "Batch 7 input shapes: input_ids=torch.Size([4, 512]), attention_mask=torch.Size([4, 512])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Predicting:   5%|▍         | 8/164 [00:28<10:14,  3.94s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "GPU memory after batch 7: 14.60 GB\n",
      "Batch 8 input shapes: input_ids=torch.Size([4, 512]), attention_mask=torch.Size([4, 512])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Predicting:   5%|▌         | 9/164 [00:32<10:21,  4.01s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "GPU memory after batch 8: 14.60 GB\n",
      "Batch 9 input shapes: input_ids=torch.Size([4, 512]), attention_mask=torch.Size([4, 512])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Predicting:   6%|▌         | 10/164 [00:36<10:25,  4.06s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "GPU memory after batch 9: 14.60 GB\n",
      "Batch 10 input shapes: input_ids=torch.Size([4, 512]), attention_mask=torch.Size([4, 512])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Predicting:   7%|▋         | 11/164 [00:40<10:26,  4.09s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "批次 10: 原始预测: ['Yes\\n\\nIs \"the', 'in control as QB Peyton', 'Yes\\n\\nIs \"the', 'to run the ball well']\n",
      "GPU memory after batch 10: 14.60 GB\n",
      "Batch 11 input shapes: input_ids=torch.Size([4, 512]), attention_mask=torch.Size([4, 512])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Predicting:   7%|▋         | 12/164 [00:45<10:25,  4.12s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "GPU memory after batch 11: 14.60 GB\n",
      "Batch 12 input shapes: input_ids=torch.Size([4, 512]), attention_mask=torch.Size([4, 512])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Predicting:   8%|▊         | 13/164 [00:49<10:24,  4.13s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "GPU memory after batch 12: 14.60 GB\n",
      "Batch 13 input shapes: input_ids=torch.Size([4, 512]), attention_mask=torch.Size([4, 512])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Predicting:   9%|▊         | 14/164 [00:53<10:21,  4.14s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "GPU memory after batch 13: 14.60 GB\n",
      "Batch 14 input shapes: input_ids=torch.Size([4, 512]), attention_mask=torch.Size([4, 512])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Predicting:   9%|▉         | 15/164 [00:57<10:18,  4.15s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "GPU memory after batch 14: 14.60 GB\n",
      "Batch 15 input shapes: input_ids=torch.Size([4, 512]), attention_mask=torch.Size([4, 512])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Predicting:  10%|▉         | 16/164 [01:01<10:15,  4.16s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "GPU memory after batch 15: 14.60 GB\n",
      "Batch 16 input shapes: input_ids=torch.Size([4, 512]), attention_mask=torch.Size([4, 512])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Predicting:  10%|█         | 17/164 [01:05<10:11,  4.16s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "GPU memory after batch 16: 14.60 GB\n",
      "Batch 17 input shapes: input_ids=torch.Size([4, 512]), attention_mask=torch.Size([4, 512])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Predicting:  11%|█         | 18/164 [01:10<10:07,  4.16s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "GPU memory after batch 17: 14.60 GB\n",
      "Batch 18 input shapes: input_ids=torch.Size([4, 512]), attention_mask=torch.Size([4, 512])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Predicting:  12%|█▏        | 19/164 [01:14<10:04,  4.17s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "GPU memory after batch 18: 14.60 GB\n",
      "Batch 19 input shapes: input_ids=torch.Size([4, 512]), attention_mask=torch.Size([4, 512])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Predicting:  12%|█▏        | 20/164 [01:18<10:00,  4.17s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "GPU memory after batch 19: 14.60 GB\n",
      "Batch 20 input shapes: input_ids=torch.Size([4, 512]), attention_mask=torch.Size([4, 512])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Predicting:  13%|█▎        | 21/164 [01:22<09:56,  4.17s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "批次 20: 原始预测: ['Yes\\n\\nIs \"the', 'continued to dominate the line', 'Yes\\n\\nIs \"the', '-yard run play. However']\n",
      "GPU memory after batch 20: 14.60 GB\n",
      "Batch 21 input shapes: input_ids=torch.Size([4, 512]), attention_mask=torch.Size([4, 512])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Predicting:  13%|█▎        | 22/164 [01:26<09:51,  4.17s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "GPU memory after batch 21: 14.60 GB\n",
      "Batch 22 input shapes: input_ids=torch.Size([4, 512]), attention_mask=torch.Size([4, 512])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Predicting:  14%|█▍        | 23/164 [01:30<09:47,  4.17s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "GPU memory after batch 22: 14.60 GB\n",
      "Batch 23 input shapes: input_ids=torch.Size([4, 512]), attention_mask=torch.Size([4, 512])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Predicting:  15%|█▍        | 24/164 [01:35<09:43,  4.17s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "GPU memory after batch 23: 14.60 GB\n",
      "Batch 24 input shapes: input_ids=torch.Size([4, 512]), attention_mask=torch.Size([4, 512])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Predicting:  15%|█▌        | 25/164 [01:39<09:39,  4.17s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "GPU memory after batch 24: 14.60 GB\n",
      "Batch 25 input shapes: input_ids=torch.Size([4, 512]), attention_mask=torch.Size([4, 512])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Predicting:  16%|█▌        | 26/164 [01:43<09:35,  4.17s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "GPU memory after batch 25: 14.60 GB\n",
      "Batch 26 input shapes: input_ids=torch.Size([4, 512]), attention_mask=torch.Size([4, 512])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Predicting:  16%|█▋        | 27/164 [01:47<09:31,  4.17s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "GPU memory after batch 26: 14.60 GB\n",
      "Batch 27 input shapes: input_ids=torch.Size([4, 512]), attention_mask=torch.Size([4, 512])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Predicting:  17%|█▋        | 28/164 [01:51<09:27,  4.17s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "GPU memory after batch 27: 14.60 GB\n",
      "Batch 28 input shapes: input_ids=torch.Size([4, 512]), attention_mask=torch.Size([4, 512])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Predicting:  18%|█▊        | 29/164 [01:55<09:22,  4.17s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "GPU memory after batch 28: 14.60 GB\n",
      "Batch 29 input shapes: input_ids=torch.Size([4, 512]), attention_mask=torch.Size([4, 512])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Predicting:  18%|█▊        | 30/164 [02:00<09:18,  4.17s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "GPU memory after batch 29: 14.60 GB\n",
      "Batch 30 input shapes: input_ids=torch.Size([4, 512]), attention_mask=torch.Size([4, 512])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Predicting:  19%|█▉        | 31/164 [02:04<09:14,  4.17s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "批次 30: 原始预测: ['Yes\\n\\nIs \"the', '. The Bills then drove', 'Yes\\n\\nIs \"the', '-16. The']\n",
      "GPU memory after batch 30: 14.60 GB\n",
      "Batch 31 input shapes: input_ids=torch.Size([4, 512]), attention_mask=torch.Size([4, 512])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Predicting:  20%|█▉        | 32/164 [02:08<09:10,  4.17s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "GPU memory after batch 31: 14.60 GB\n",
      "Batch 32 input shapes: input_ids=torch.Size([4, 512]), attention_mask=torch.Size([4, 512])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Predicting:  20%|██        | 33/164 [02:12<09:06,  4.17s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "GPU memory after batch 32: 14.60 GB\n",
      "Batch 33 input shapes: input_ids=torch.Size([4, 512]), attention_mask=torch.Size([4, 512])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Predicting:  21%|██        | 34/164 [02:16<09:01,  4.17s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "GPU memory after batch 33: 14.60 GB\n",
      "Batch 34 input shapes: input_ids=torch.Size([4, 512]), attention_mask=torch.Size([4, 512])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Predicting:  21%|██▏       | 35/164 [02:20<08:57,  4.17s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "GPU memory after batch 34: 14.60 GB\n",
      "Batch 35 input shapes: input_ids=torch.Size([4, 512]), attention_mask=torch.Size([4, 512])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Predicting:  22%|██▏       | 36/164 [02:25<08:53,  4.17s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "GPU memory after batch 35: 14.60 GB\n",
      "Batch 36 input shapes: input_ids=torch.Size([4, 512]), attention_mask=torch.Size([4, 512])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Predicting:  23%|██▎       | 37/164 [02:29<08:49,  4.17s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "GPU memory after batch 36: 14.60 GB\n",
      "Batch 37 input shapes: input_ids=torch.Size([4, 512]), attention_mask=torch.Size([4, 512])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Predicting:  23%|██▎       | 38/164 [02:33<08:45,  4.17s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "GPU memory after batch 37: 14.60 GB\n",
      "Batch 38 input shapes: input_ids=torch.Size([4, 512]), attention_mask=torch.Size([4, 512])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Predicting:  24%|██▍       | 39/164 [02:37<08:40,  4.17s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "GPU memory after batch 38: 14.60 GB\n",
      "Batch 39 input shapes: input_ids=torch.Size([4, 512]), attention_mask=torch.Size([4, 512])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Predicting:  24%|██▍       | 40/164 [02:41<08:36,  4.17s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "GPU memory after batch 39: 14.60 GB\n",
      "Batch 40 input shapes: input_ids=torch.Size([4, 512]), attention_mask=torch.Size([4, 512])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Predicting:  25%|██▌       | 41/164 [02:45<08:32,  4.17s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "批次 40: 原始预测: ['Yes\\n\\nIs \"the', 'from their own 1', 'Yes\\n\\nIs \"the', ', the Packers fell to']\n",
      "GPU memory after batch 40: 14.60 GB\n",
      "Batch 41 input shapes: input_ids=torch.Size([4, 512]), attention_mask=torch.Size([4, 512])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Predicting:  26%|██▌       | 42/164 [02:50<08:28,  4.17s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "GPU memory after batch 41: 14.60 GB\n",
      "Batch 42 input shapes: input_ids=torch.Size([4, 512]), attention_mask=torch.Size([4, 512])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Predicting:  26%|██▌       | 43/164 [02:54<08:24,  4.17s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "GPU memory after batch 42: 14.60 GB\n",
      "Batch 43 input shapes: input_ids=torch.Size([4, 512]), attention_mask=torch.Size([4, 512])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Predicting:  27%|██▋       | 44/164 [02:58<08:20,  4.17s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "GPU memory after batch 43: 14.60 GB\n",
      "Batch 44 input shapes: input_ids=torch.Size([4, 512]), attention_mask=torch.Size([4, 512])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Predicting:  27%|██▋       | 45/164 [03:02<08:16,  4.17s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "GPU memory after batch 44: 14.60 GB\n",
      "Batch 45 input shapes: input_ids=torch.Size([4, 512]), attention_mask=torch.Size([4, 512])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Predicting:  28%|██▊       | 46/164 [03:06<08:12,  4.17s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "GPU memory after batch 45: 14.60 GB\n",
      "Batch 46 input shapes: input_ids=torch.Size([4, 512]), attention_mask=torch.Size([4, 512])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Predicting:  29%|██▊       | 47/164 [03:10<08:07,  4.17s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "GPU memory after batch 46: 14.60 GB\n",
      "Batch 47 input shapes: input_ids=torch.Size([4, 512]), attention_mask=torch.Size([4, 512])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Predicting:  29%|██▉       | 48/164 [03:15<08:03,  4.17s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "GPU memory after batch 47: 14.60 GB\n",
      "Batch 48 input shapes: input_ids=torch.Size([4, 512]), attention_mask=torch.Size([4, 512])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Predicting:  30%|██▉       | 49/164 [03:19<07:59,  4.17s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "GPU memory after batch 48: 14.60 GB\n",
      "Batch 49 input shapes: input_ids=torch.Size([4, 512]), attention_mask=torch.Size([4, 512])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Predicting:  30%|███       | 50/164 [03:23<07:55,  4.17s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "GPU memory after batch 49: 14.60 GB\n",
      "Batch 50 input shapes: input_ids=torch.Size([4, 512]), attention_mask=torch.Size([4, 512])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Predicting:  31%|███       | 51/164 [03:27<07:51,  4.17s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "批次 50: 原始预测: ['Yes\\n\\nIs \"the', 'the half was a back', 'Yes\\n\\nIs \"the', '2003']\n",
      "GPU memory after batch 50: 14.60 GB\n",
      "Batch 51 input shapes: input_ids=torch.Size([4, 512]), attention_mask=torch.Size([4, 512])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Predicting:  32%|███▏      | 52/164 [03:31<07:46,  4.17s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "GPU memory after batch 51: 14.60 GB\n",
      "Batch 52 input shapes: input_ids=torch.Size([4, 512]), attention_mask=torch.Size([4, 512])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Predicting:  32%|███▏      | 53/164 [03:35<07:42,  4.17s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "GPU memory after batch 52: 14.60 GB\n",
      "Batch 53 input shapes: input_ids=torch.Size([4, 512]), attention_mask=torch.Size([4, 512])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Predicting:  33%|███▎      | 54/164 [03:40<07:38,  4.17s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "GPU memory after batch 53: 14.60 GB\n",
      "Batch 54 input shapes: input_ids=torch.Size([4, 512]), attention_mask=torch.Size([4, 512])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Predicting:  34%|███▎      | 55/164 [03:44<07:34,  4.17s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "GPU memory after batch 54: 14.60 GB\n",
      "Batch 55 input shapes: input_ids=torch.Size([4, 512]), attention_mask=torch.Size([4, 512])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Predicting:  34%|███▍      | 56/164 [03:48<07:30,  4.17s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "GPU memory after batch 55: 14.60 GB\n",
      "Batch 56 input shapes: input_ids=torch.Size([4, 512]), attention_mask=torch.Size([4, 512])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Predicting:  35%|███▍      | 57/164 [03:52<07:25,  4.17s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "GPU memory after batch 56: 14.60 GB\n",
      "Batch 57 input shapes: input_ids=torch.Size([4, 512]), attention_mask=torch.Size([4, 512])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Predicting:  35%|███▌      | 58/164 [03:56<07:21,  4.17s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "GPU memory after batch 57: 14.60 GB\n",
      "Batch 58 input shapes: input_ids=torch.Size([4, 512]), attention_mask=torch.Size([4, 512])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Predicting:  36%|███▌      | 59/164 [04:00<07:17,  4.17s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "GPU memory after batch 58: 14.60 GB\n",
      "Batch 59 input shapes: input_ids=torch.Size([4, 512]), attention_mask=torch.Size([4, 512])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Predicting:  37%|███▋      | 60/164 [04:05<07:13,  4.17s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "GPU memory after batch 59: 14.60 GB\n",
      "Batch 60 input shapes: input_ids=torch.Size([4, 512]), attention_mask=torch.Size([4, 512])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Predicting:  37%|███▋      | 61/164 [04:09<07:09,  4.17s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "批次 60: 原始预测: ['Yes\\n\\nIs \"the', '10 yards to Jeff', 'Yes\\n\\nIs \"the', 'of the Lions each had']\n",
      "GPU memory after batch 60: 14.60 GB\n",
      "Batch 61 input shapes: input_ids=torch.Size([4, 512]), attention_mask=torch.Size([4, 512])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Predicting:  38%|███▊      | 62/164 [04:13<07:05,  4.17s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "GPU memory after batch 61: 14.60 GB\n",
      "Batch 62 input shapes: input_ids=torch.Size([4, 512]), attention_mask=torch.Size([4, 512])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Predicting:  38%|███▊      | 63/164 [04:17<07:00,  4.17s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "GPU memory after batch 62: 14.60 GB\n",
      "Batch 63 input shapes: input_ids=torch.Size([4, 512]), attention_mask=torch.Size([4, 512])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Predicting:  39%|███▉      | 64/164 [04:21<06:56,  4.17s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "GPU memory after batch 63: 14.60 GB\n",
      "Batch 64 input shapes: input_ids=torch.Size([4, 512]), attention_mask=torch.Size([4, 512])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Predicting:  40%|███▉      | 65/164 [04:25<06:52,  4.17s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "GPU memory after batch 64: 14.60 GB\n",
      "Batch 65 input shapes: input_ids=torch.Size([4, 512]), attention_mask=torch.Size([4, 512])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Predicting:  40%|████      | 66/164 [04:30<06:48,  4.17s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "GPU memory after batch 65: 14.60 GB\n",
      "Batch 66 input shapes: input_ids=torch.Size([4, 512]), attention_mask=torch.Size([4, 512])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Predicting:  41%|████      | 67/164 [04:34<06:44,  4.17s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "GPU memory after batch 66: 14.60 GB\n",
      "Batch 67 input shapes: input_ids=torch.Size([4, 512]), attention_mask=torch.Size([4, 512])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Predicting:  41%|████▏     | 68/164 [04:38<06:40,  4.17s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "GPU memory after batch 67: 14.60 GB\n",
      "Batch 68 input shapes: input_ids=torch.Size([4, 512]), attention_mask=torch.Size([4, 512])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Predicting:  42%|████▏     | 69/164 [04:42<06:35,  4.17s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "GPU memory after batch 68: 14.60 GB\n",
      "Batch 69 input shapes: input_ids=torch.Size([4, 512]), attention_mask=torch.Size([4, 512])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Predicting:  43%|████▎     | 70/164 [04:46<06:31,  4.17s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "GPU memory after batch 69: 14.60 GB\n",
      "Batch 70 input shapes: input_ids=torch.Size([4, 512]), attention_mask=torch.Size([4, 512])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Predicting:  43%|████▎     | 71/164 [04:50<06:27,  4.17s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "批次 70: 原始预测: ['Yes\\n\\nIs \"the', '4th and', 'Yes\\n\\nIs \"the', '24 first downs']\n",
      "GPU memory after batch 70: 14.60 GB\n",
      "Batch 71 input shapes: input_ids=torch.Size([4, 512]), attention_mask=torch.Size([4, 512])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Predicting:  44%|████▍     | 72/164 [04:55<06:23,  4.17s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "GPU memory after batch 71: 14.60 GB\n",
      "Batch 72 input shapes: input_ids=torch.Size([4, 512]), attention_mask=torch.Size([4, 512])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Predicting:  45%|████▍     | 73/164 [04:59<06:19,  4.17s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "GPU memory after batch 72: 14.60 GB\n",
      "Batch 73 input shapes: input_ids=torch.Size([4, 512]), attention_mask=torch.Size([4, 512])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Predicting:  45%|████▌     | 74/164 [05:03<06:15,  4.17s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "GPU memory after batch 73: 14.60 GB\n",
      "Batch 74 input shapes: input_ids=torch.Size([4, 512]), attention_mask=torch.Size([4, 512])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Predicting:  46%|████▌     | 75/164 [05:07<06:11,  4.17s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "GPU memory after batch 74: 14.60 GB\n",
      "Batch 75 input shapes: input_ids=torch.Size([4, 512]), attention_mask=torch.Size([4, 512])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Predicting:  46%|████▋     | 76/164 [05:11<06:06,  4.17s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "GPU memory after batch 75: 14.60 GB\n",
      "Batch 76 input shapes: input_ids=torch.Size([4, 512]), attention_mask=torch.Size([4, 512])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Predicting:  47%|████▋     | 77/164 [05:15<06:02,  4.17s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "GPU memory after batch 76: 14.60 GB\n",
      "Batch 77 input shapes: input_ids=torch.Size([4, 512]), attention_mask=torch.Size([4, 512])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Predicting:  48%|████▊     | 78/164 [05:20<05:58,  4.17s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "GPU memory after batch 77: 14.60 GB\n",
      "Batch 78 input shapes: input_ids=torch.Size([4, 512]), attention_mask=torch.Size([4, 512])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Predicting:  48%|████▊     | 79/164 [05:24<05:54,  4.17s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "GPU memory after batch 78: 14.60 GB\n",
      "Batch 79 input shapes: input_ids=torch.Size([4, 512]), attention_mask=torch.Size([4, 512])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Predicting:  49%|████▉     | 80/164 [05:28<05:49,  4.17s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "GPU memory after batch 79: 14.60 GB\n",
      "Batch 80 input shapes: input_ids=torch.Size([4, 512]), attention_mask=torch.Size([4, 512])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Predicting:  49%|████▉     | 81/164 [05:32<05:45,  4.17s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "批次 80: 原始预测: ['Yes\\n\\nIs \"the', '1:11 left', 'Yes\\n\\nIs \"the', 'game with a 1']\n",
      "GPU memory after batch 80: 14.60 GB\n",
      "Batch 81 input shapes: input_ids=torch.Size([4, 512]), attention_mask=torch.Size([4, 512])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Predicting:  50%|█████     | 82/164 [05:36<05:41,  4.17s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "GPU memory after batch 81: 14.60 GB\n",
      "Batch 82 input shapes: input_ids=torch.Size([4, 512]), attention_mask=torch.Size([4, 512])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Predicting:  51%|█████     | 83/164 [05:40<05:37,  4.16s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "GPU memory after batch 82: 14.60 GB\n",
      "Batch 83 input shapes: input_ids=torch.Size([4, 512]), attention_mask=torch.Size([4, 512])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Predicting:  51%|█████     | 84/164 [05:45<05:33,  4.16s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "GPU memory after batch 83: 14.60 GB\n",
      "Batch 84 input shapes: input_ids=torch.Size([4, 512]), attention_mask=torch.Size([4, 512])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Predicting:  52%|█████▏    | 85/164 [05:49<05:28,  4.16s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "GPU memory after batch 84: 14.60 GB\n",
      "Batch 85 input shapes: input_ids=torch.Size([4, 512]), attention_mask=torch.Size([4, 512])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Predicting:  52%|█████▏    | 86/164 [05:53<05:24,  4.16s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "GPU memory after batch 85: 14.60 GB\n",
      "Batch 86 input shapes: input_ids=torch.Size([4, 512]), attention_mask=torch.Size([4, 512])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Predicting:  53%|█████▎    | 87/164 [05:57<05:20,  4.17s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "GPU memory after batch 86: 14.60 GB\n",
      "Batch 87 input shapes: input_ids=torch.Size([4, 512]), attention_mask=torch.Size([4, 512])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Predicting:  54%|█████▎    | 88/164 [06:01<05:16,  4.16s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "GPU memory after batch 87: 14.60 GB\n",
      "Batch 88 input shapes: input_ids=torch.Size([4, 512]), attention_mask=torch.Size([4, 512])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Predicting:  54%|█████▍    | 89/164 [06:05<05:12,  4.17s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "GPU memory after batch 88: 14.60 GB\n",
      "Batch 89 input shapes: input_ids=torch.Size([4, 512]), attention_mask=torch.Size([4, 512])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Predicting:  55%|█████▍    | 90/164 [06:10<05:08,  4.17s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "GPU memory after batch 89: 14.60 GB\n",
      "Batch 90 input shapes: input_ids=torch.Size([4, 512]), attention_mask=torch.Size([4, 512])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Predicting:  55%|█████▌    | 91/164 [06:14<05:04,  4.17s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "批次 90: 原始预测: ['Yes\\n\\nIs \"the', 'Carter were all on the', 'Yes\\n\\nIs \"the', 'Raiders took the knee and']\n",
      "GPU memory after batch 90: 14.60 GB\n",
      "Batch 91 input shapes: input_ids=torch.Size([4, 512]), attention_mask=torch.Size([4, 512])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Predicting:  56%|█████▌    | 92/164 [06:18<05:00,  4.17s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "GPU memory after batch 91: 14.60 GB\n",
      "Batch 92 input shapes: input_ids=torch.Size([4, 512]), attention_mask=torch.Size([4, 512])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Predicting:  57%|█████▋    | 93/164 [06:22<04:55,  4.17s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "GPU memory after batch 92: 14.60 GB\n",
      "Batch 93 input shapes: input_ids=torch.Size([4, 512]), attention_mask=torch.Size([4, 512])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Predicting:  57%|█████▋    | 94/164 [06:26<04:51,  4.17s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "GPU memory after batch 93: 14.60 GB\n",
      "Batch 94 input shapes: input_ids=torch.Size([4, 512]), attention_mask=torch.Size([4, 512])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Predicting:  58%|█████▊    | 95/164 [06:30<04:47,  4.17s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "GPU memory after batch 94: 14.60 GB\n",
      "Batch 95 input shapes: input_ids=torch.Size([4, 512]), attention_mask=torch.Size([4, 512])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Predicting:  59%|█████▊    | 96/164 [06:35<04:43,  4.17s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "GPU memory after batch 95: 14.60 GB\n",
      "Batch 96 input shapes: input_ids=torch.Size([4, 512]), attention_mask=torch.Size([4, 512])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Predicting:  59%|█████▉    | 97/164 [06:39<04:39,  4.17s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "GPU memory after batch 96: 14.60 GB\n",
      "Batch 97 input shapes: input_ids=torch.Size([4, 512]), attention_mask=torch.Size([4, 512])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Predicting:  60%|█████▉    | 98/164 [06:43<04:34,  4.17s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "GPU memory after batch 97: 14.60 GB\n",
      "Batch 98 input shapes: input_ids=torch.Size([4, 512]), attention_mask=torch.Size([4, 512])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Predicting:  60%|██████    | 99/164 [06:47<04:30,  4.17s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "GPU memory after batch 98: 14.60 GB\n",
      "Batch 99 input shapes: input_ids=torch.Size([4, 512]), attention_mask=torch.Size([4, 512])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Predicting:  61%|██████    | 100/164 [06:51<04:26,  4.17s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "GPU memory after batch 99: 14.60 GB\n",
      "Batch 100 input shapes: input_ids=torch.Size([4, 512]), attention_mask=torch.Size([4, 512])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Predicting:  62%|██████▏   | 101/164 [06:55<04:22,  4.17s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "批次 100: 原始预测: ['Yes\\n\\nIs \"the', 'in a 14', 'Yes\\n\\nIs \"the', 'a 14-yard']\n",
      "GPU memory after batch 100: 14.60 GB\n",
      "Batch 101 input shapes: input_ids=torch.Size([4, 512]), attention_mask=torch.Size([4, 512])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Predicting:  62%|██████▏   | 102/164 [07:00<04:18,  4.17s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "GPU memory after batch 101: 14.60 GB\n",
      "Batch 102 input shapes: input_ids=torch.Size([4, 512]), attention_mask=torch.Size([4, 512])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Predicting:  63%|██████▎   | 103/164 [07:04<04:14,  4.17s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "GPU memory after batch 102: 14.60 GB\n",
      "Batch 103 input shapes: input_ids=torch.Size([4, 512]), attention_mask=torch.Size([4, 512])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Predicting:  63%|██████▎   | 104/164 [07:08<04:09,  4.16s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "GPU memory after batch 103: 14.60 GB\n",
      "Batch 104 input shapes: input_ids=torch.Size([4, 512]), attention_mask=torch.Size([4, 512])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Predicting:  64%|██████▍   | 105/164 [07:12<04:05,  4.17s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "GPU memory after batch 104: 14.60 GB\n",
      "Batch 105 input shapes: input_ids=torch.Size([4, 512]), attention_mask=torch.Size([4, 512])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Predicting:  65%|██████▍   | 106/164 [07:16<04:01,  4.17s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "GPU memory after batch 105: 14.60 GB\n",
      "Batch 106 input shapes: input_ids=torch.Size([4, 512]), attention_mask=torch.Size([4, 512])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Predicting:  65%|██████▌   | 107/164 [07:20<03:57,  4.17s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "GPU memory after batch 106: 14.60 GB\n",
      "Batch 107 input shapes: input_ids=torch.Size([4, 512]), attention_mask=torch.Size([4, 512])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Predicting:  66%|██████▌   | 108/164 [07:25<03:53,  4.17s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "GPU memory after batch 107: 14.60 GB\n",
      "Batch 108 input shapes: input_ids=torch.Size([4, 512]), attention_mask=torch.Size([4, 512])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Predicting:  66%|██████▋   | 109/164 [07:29<03:49,  4.17s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "GPU memory after batch 108: 14.60 GB\n",
      "Batch 109 input shapes: input_ids=torch.Size([4, 512]), attention_mask=torch.Size([4, 512])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Predicting:  67%|██████▋   | 110/164 [07:33<03:45,  4.17s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "GPU memory after batch 109: 14.60 GB\n",
      "Batch 110 input shapes: input_ids=torch.Size([4, 512]), attention_mask=torch.Size([4, 512])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Predicting:  68%|██████▊   | 111/164 [07:37<03:40,  4.17s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "批次 110: 原始预测: ['Yes\\n\\nIs \"the', 'kiemedei and forced', 'Yes\\n\\nIs \"the', 'Floyd, resulting in a']\n",
      "GPU memory after batch 110: 14.60 GB\n",
      "Batch 111 input shapes: input_ids=torch.Size([4, 512]), attention_mask=torch.Size([4, 512])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Predicting:  68%|██████▊   | 112/164 [07:41<03:36,  4.17s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "GPU memory after batch 111: 14.60 GB\n",
      "Batch 112 input shapes: input_ids=torch.Size([4, 512]), attention_mask=torch.Size([4, 512])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Predicting:  69%|██████▉   | 113/164 [07:45<03:32,  4.17s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "GPU memory after batch 112: 14.60 GB\n",
      "Batch 113 input shapes: input_ids=torch.Size([4, 512]), attention_mask=torch.Size([4, 512])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Predicting:  70%|██████▉   | 114/164 [07:50<03:28,  4.17s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "GPU memory after batch 113: 14.60 GB\n",
      "Batch 114 input shapes: input_ids=torch.Size([4, 512]), attention_mask=torch.Size([4, 512])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Predicting:  70%|███████   | 115/164 [07:54<03:24,  4.17s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "GPU memory after batch 114: 14.60 GB\n",
      "Batch 115 input shapes: input_ids=torch.Size([4, 512]), attention_mask=torch.Size([4, 512])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Predicting:  71%|███████   | 116/164 [07:58<03:20,  4.17s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "GPU memory after batch 115: 14.60 GB\n",
      "Batch 116 input shapes: input_ids=torch.Size([4, 512]), attention_mask=torch.Size([4, 512])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Predicting:  71%|███████▏  | 117/164 [08:02<03:15,  4.17s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "GPU memory after batch 116: 14.60 GB\n",
      "Batch 117 input shapes: input_ids=torch.Size([4, 512]), attention_mask=torch.Size([4, 512])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Predicting:  72%|███████▏  | 118/164 [08:06<03:11,  4.17s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "GPU memory after batch 117: 14.60 GB\n",
      "Batch 118 input shapes: input_ids=torch.Size([4, 512]), attention_mask=torch.Size([4, 512])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Predicting:  73%|███████▎  | 119/164 [08:11<03:07,  4.17s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "GPU memory after batch 118: 14.60 GB\n",
      "Batch 119 input shapes: input_ids=torch.Size([4, 512]), attention_mask=torch.Size([4, 512])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Predicting:  73%|███████▎  | 120/164 [08:15<03:03,  4.17s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "GPU memory after batch 119: 14.60 GB\n",
      "Batch 120 input shapes: input_ids=torch.Size([4, 512]), attention_mask=torch.Size([4, 512])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Predicting:  74%|███████▍  | 121/164 [08:19<02:59,  4.17s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "批次 120: 原始预测: ['No\\n\\nQuestion: Is', 'icius for a touchdown', 'Yes\\n\\nIs \"the', 'he was already out of']\n",
      "GPU memory after batch 120: 14.60 GB\n",
      "Batch 121 input shapes: input_ids=torch.Size([4, 512]), attention_mask=torch.Size([4, 512])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Predicting:  74%|███████▍  | 122/164 [08:23<02:55,  4.17s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "GPU memory after batch 121: 14.60 GB\n",
      "Batch 122 input shapes: input_ids=torch.Size([4, 512]), attention_mask=torch.Size([4, 512])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Predicting:  75%|███████▌  | 123/164 [08:27<02:50,  4.17s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "GPU memory after batch 122: 14.60 GB\n",
      "Batch 123 input shapes: input_ids=torch.Size([4, 512]), attention_mask=torch.Size([4, 512])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Predicting:  76%|███████▌  | 124/164 [08:31<02:46,  4.17s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "GPU memory after batch 123: 14.60 GB\n",
      "Batch 124 input shapes: input_ids=torch.Size([4, 512]), attention_mask=torch.Size([4, 512])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Predicting:  76%|███████▌  | 125/164 [08:36<02:42,  4.17s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "GPU memory after batch 124: 14.60 GB\n",
      "Batch 125 input shapes: input_ids=torch.Size([4, 512]), attention_mask=torch.Size([4, 512])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Predicting:  77%|███████▋  | 126/164 [08:40<02:38,  4.17s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "GPU memory after batch 125: 14.60 GB\n",
      "Batch 126 input shapes: input_ids=torch.Size([4, 512]), attention_mask=torch.Size([4, 512])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Predicting:  77%|███████▋  | 127/164 [08:44<02:34,  4.17s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "GPU memory after batch 126: 14.60 GB\n",
      "Batch 127 input shapes: input_ids=torch.Size([4, 512]), attention_mask=torch.Size([4, 512])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Predicting:  78%|███████▊  | 128/164 [08:48<02:30,  4.17s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "GPU memory after batch 127: 14.60 GB\n",
      "Batch 128 input shapes: input_ids=torch.Size([4, 512]), attention_mask=torch.Size([4, 512])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Predicting:  79%|███████▊  | 129/164 [08:52<02:25,  4.17s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "GPU memory after batch 128: 14.60 GB\n",
      "Batch 129 input shapes: input_ids=torch.Size([4, 512]), attention_mask=torch.Size([4, 512])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Predicting:  79%|███████▉  | 130/164 [08:56<02:21,  4.17s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "GPU memory after batch 129: 14.60 GB\n",
      "Batch 130 input shapes: input_ids=torch.Size([4, 512]), attention_mask=torch.Size([4, 512])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Predicting:  80%|███████▉  | 131/164 [09:01<02:17,  4.17s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "批次 130: 原始预测: ['Yes\\n\\nIs \"the', '-yard line. On the', 'Yes\\n\\nIs \"the', 'was ruled out of bounds']\n",
      "GPU memory after batch 130: 14.60 GB\n",
      "Batch 131 input shapes: input_ids=torch.Size([4, 512]), attention_mask=torch.Size([4, 512])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Predicting:  80%|████████  | 132/164 [09:05<02:13,  4.17s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "GPU memory after batch 131: 14.60 GB\n",
      "Batch 132 input shapes: input_ids=torch.Size([4, 512]), attention_mask=torch.Size([4, 512])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Predicting:  81%|████████  | 133/164 [09:09<02:09,  4.17s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "GPU memory after batch 132: 14.60 GB\n",
      "Batch 133 input shapes: input_ids=torch.Size([4, 512]), attention_mask=torch.Size([4, 512])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Predicting:  82%|████████▏ | 134/164 [09:13<02:05,  4.17s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "GPU memory after batch 133: 14.60 GB\n",
      "Batch 134 input shapes: input_ids=torch.Size([4, 512]), attention_mask=torch.Size([4, 512])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Predicting:  82%|████████▏ | 135/164 [09:17<02:00,  4.17s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "GPU memory after batch 134: 14.60 GB\n",
      "Batch 135 input shapes: input_ids=torch.Size([4, 512]), attention_mask=torch.Size([4, 512])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Predicting:  83%|████████▎ | 136/164 [09:21<01:56,  4.17s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "GPU memory after batch 135: 14.60 GB\n",
      "Batch 136 input shapes: input_ids=torch.Size([4, 512]), attention_mask=torch.Size([4, 512])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Predicting:  84%|████████▎ | 137/164 [09:26<01:52,  4.17s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "GPU memory after batch 136: 14.60 GB\n",
      "Batch 137 input shapes: input_ids=torch.Size([4, 512]), attention_mask=torch.Size([4, 512])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Predicting:  84%|████████▍ | 138/164 [09:30<01:48,  4.17s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "GPU memory after batch 137: 14.60 GB\n",
      "Batch 138 input shapes: input_ids=torch.Size([4, 512]), attention_mask=torch.Size([4, 512])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Predicting:  85%|████████▍ | 139/164 [09:34<01:44,  4.17s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "GPU memory after batch 138: 14.60 GB\n",
      "Batch 139 input shapes: input_ids=torch.Size([4, 512]), attention_mask=torch.Size([4, 512])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Predicting:  85%|████████▌ | 140/164 [09:38<01:40,  4.17s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "GPU memory after batch 139: 14.60 GB\n",
      "Batch 140 input shapes: input_ids=torch.Size([4, 512]), attention_mask=torch.Size([4, 512])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Predicting:  86%|████████▌ | 141/164 [09:42<01:35,  4.17s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "批次 140: 原始预测: ['Yes\\n\\nIs \"the', 'Brandon Lloyd) (CH', 'Yes\\n\\nIs \"the', '12:00']\n",
      "GPU memory after batch 140: 14.60 GB\n",
      "Batch 141 input shapes: input_ids=torch.Size([4, 239]), attention_mask=torch.Size([4, 239])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Predicting:  87%|████████▋ | 142/164 [09:45<01:21,  3.68s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "GPU memory after batch 141: 14.60 GB\n",
      "Batch 142 input shapes: input_ids=torch.Size([4, 238]), attention_mask=torch.Size([4, 238])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Predicting:  87%|████████▋ | 143/164 [09:47<01:10,  3.34s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "GPU memory after batch 142: 14.60 GB\n",
      "Batch 143 input shapes: input_ids=torch.Size([4, 239]), attention_mask=torch.Size([4, 239])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Predicting:  88%|████████▊ | 144/164 [09:50<01:02,  3.10s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "GPU memory after batch 143: 14.60 GB\n",
      "Batch 144 input shapes: input_ids=torch.Size([4, 512]), attention_mask=torch.Size([4, 512])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Predicting:  88%|████████▊ | 145/164 [09:54<01:05,  3.42s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "GPU memory after batch 144: 14.60 GB\n",
      "Batch 145 input shapes: input_ids=torch.Size([4, 411]), attention_mask=torch.Size([4, 411])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Predicting:  89%|████████▉ | 146/164 [09:57<01:01,  3.41s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "GPU memory after batch 145: 14.60 GB\n",
      "Batch 146 input shapes: input_ids=torch.Size([4, 441]), attention_mask=torch.Size([4, 441])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Predicting:  90%|████████▉ | 147/164 [10:01<00:58,  3.45s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "GPU memory after batch 146: 14.60 GB\n",
      "Batch 147 input shapes: input_ids=torch.Size([4, 403]), attention_mask=torch.Size([4, 403])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Predicting:  90%|█████████ | 148/164 [10:04<00:54,  3.42s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "GPU memory after batch 147: 14.60 GB\n",
      "Batch 148 input shapes: input_ids=torch.Size([4, 377]), attention_mask=torch.Size([4, 377])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Predicting:  91%|█████████ | 149/164 [10:08<00:52,  3.49s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "GPU memory after batch 148: 14.60 GB\n",
      "Batch 149 input shapes: input_ids=torch.Size([4, 219]), attention_mask=torch.Size([4, 219])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Predicting:  91%|█████████▏| 150/164 [10:10<00:44,  3.15s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "GPU memory after batch 149: 14.60 GB\n",
      "Batch 150 input shapes: input_ids=torch.Size([4, 77]), attention_mask=torch.Size([4, 77])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Predicting:  92%|█████████▏| 151/164 [10:12<00:33,  2.56s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "批次 150: 原始预测: ['Yes\\n\\nIs \"the', '<think>\\nOkay', 'Yes\\n\\nIs \"the', '<think>\\nOkay']\n",
      "GPU memory after batch 150: 14.60 GB\n",
      "Batch 151 input shapes: input_ids=torch.Size([4, 79]), attention_mask=torch.Size([4, 79])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Predicting:  93%|█████████▎| 152/164 [10:13<00:25,  2.16s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "GPU memory after batch 151: 14.60 GB\n",
      "Batch 152 input shapes: input_ids=torch.Size([4, 80]), attention_mask=torch.Size([4, 80])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Predicting:  93%|█████████▎| 153/164 [10:14<00:20,  1.87s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "GPU memory after batch 152: 14.60 GB\n",
      "Batch 153 input shapes: input_ids=torch.Size([4, 86]), attention_mask=torch.Size([4, 86])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Predicting:  94%|█████████▍| 154/164 [10:15<00:16,  1.69s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "GPU memory after batch 153: 14.60 GB\n",
      "Batch 154 input shapes: input_ids=torch.Size([4, 82]), attention_mask=torch.Size([4, 82])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Predicting:  95%|█████████▍| 155/164 [10:16<00:14,  1.57s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "GPU memory after batch 154: 14.60 GB\n",
      "Batch 155 input shapes: input_ids=torch.Size([4, 72]), attention_mask=torch.Size([4, 72])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Predicting:  95%|█████████▌| 156/164 [10:18<00:11,  1.44s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "GPU memory after batch 155: 14.60 GB\n",
      "Batch 156 input shapes: input_ids=torch.Size([4, 81]), attention_mask=torch.Size([4, 81])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Predicting:  96%|█████████▌| 157/164 [10:19<00:09,  1.39s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "GPU memory after batch 156: 14.60 GB\n",
      "Batch 157 input shapes: input_ids=torch.Size([4, 78]), attention_mask=torch.Size([4, 78])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Predicting:  96%|█████████▋| 158/164 [10:20<00:08,  1.33s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "GPU memory after batch 157: 14.60 GB\n",
      "Batch 158 input shapes: input_ids=torch.Size([4, 94]), attention_mask=torch.Size([4, 94])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Predicting:  97%|█████████▋| 159/164 [10:21<00:06,  1.33s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "GPU memory after batch 158: 14.60 GB\n",
      "Batch 159 input shapes: input_ids=torch.Size([4, 90]), attention_mask=torch.Size([4, 90])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Predicting:  98%|█████████▊| 160/164 [10:23<00:05,  1.33s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "GPU memory after batch 159: 14.60 GB\n",
      "Batch 160 input shapes: input_ids=torch.Size([4, 74]), attention_mask=torch.Size([4, 74])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Predicting:  98%|█████████▊| 161/164 [10:24<00:03,  1.29s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "批次 160: 原始预测: ['Yes\\n\\nIs \"the', '<think>\\nOkay', 'Yes\\n\\nIs \"the', '<think>\\nOkay']\n",
      "GPU memory after batch 160: 14.60 GB\n",
      "Batch 161 input shapes: input_ids=torch.Size([4, 78]), attention_mask=torch.Size([4, 78])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Predicting:  99%|█████████▉| 162/164 [10:25<00:02,  1.27s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "GPU memory after batch 161: 14.60 GB\n",
      "Batch 162 input shapes: input_ids=torch.Size([4, 79]), attention_mask=torch.Size([4, 79])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Predicting:  99%|█████████▉| 163/164 [10:26<00:01,  1.25s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "GPU memory after batch 162: 14.60 GB\n",
      "Batch 163 input shapes: input_ids=torch.Size([2, 63]), attention_mask=torch.Size([2, 63])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Predicting: 100%|██████████| 164/164 [10:27<00:00,  3.83s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "GPU memory after batch 163: 14.60 GB\n",
      "预测结果已保存到 /kaggle/working/predictions.jsonl\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "file_path = \"/kaggle/input/xiaomi-unparsed/xiaomi_unparsed_predictions.json\"  # 确认路径\n",
    "dataset = load_benedect_dataset(file_path)\n",
    "\n",
    "random_sample = random.choice(dataset)\n",
    "print(\"=== 随机样本推理 ===\")\n",
    "print(f\"Passage: {random_sample['passage']}\")\n",
    "print(f\"Number: {random_sample['number']}\")\n",
    "print(f\"Expected Answer: {random_sample['expected_answer']}\")\n",
    "print(f\"Prompt Type: {random_sample['prompt_type']}\")\n",
    "print(f\"Prompt:\\n{random_sample['prompt']}\")\n",
    "\n",
    "random_pred = predict_single(random_sample['prompt'])\n",
    "print(f\"Raw Prediction: {random_pred}\")\n",
    "\n",
    "prompts = [item['prompt'] for item in dataset]\n",
    "predictions = predict_batch(prompts, batch_size=4, max_retries=3)  # 减小批次大小\n",
    "output_file = \"/kaggle/working/predictions.jsonl\"\n",
    "save_predictions_to_jsonl(dataset, predictions, output_file)\n",
    "print(f\"预测结果已保存到 {output_file}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "from collections import Counter, defaultdict\n",
    "from typing import List, Dict, Tuple"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def parse_prediction(raw_prediction: str) -> str:\n",
    "    raw_prediction = raw_prediction.lower()\n",
    "    if 'yes' in raw_prediction:\n",
    "        return 'yes'\n",
    "    elif 'no' in raw_prediction:\n",
    "        return 'no'\n",
    "    else:\n",
    "        # print(f\"无法解析响应: {raw_prediction}\")\n",
    "        return 'generation_error'\n",
    "\n",
    "def evaluate_model(data_list: List[Dict], unparsed_output_file: str = 'unparsed_predictions.json') -> Tuple[Dict, Dict]:\n",
    "    metrics = Counter()\n",
    "    detailed_metrics = {\n",
    "        'by_domain': defaultdict(Counter),\n",
    "        'by_error_type': defaultdict(Counter),\n",
    "        'by_operation': defaultdict(Counter),\n",
    "        'by_prompt_type': defaultdict(Counter)\n",
    "    }\n",
    "    unparsed_data = {}  # 存储无法解析的样本，格式为 {id: {...}}\n",
    "    \n",
    "    for idx, item in enumerate(data_list):\n",
    "        expected = item['expected_answer'].lower()  # Yes/No 转为小写\n",
    "        pred = parse_prediction(item['raw_prediction'])\n",
    "        item['parsel_prediction'] = pred  # 保存解析结果\n",
    "        \n",
    "        # 如果无法解析，添加到 unparsed_data\n",
    "        if pred == 'generation_error':\n",
    "            # 只保存 expected_answer == \"Yes\" 的样本（错误样本）\n",
    "            if expected == 'yes':\n",
    "                sample_id = f\"unparsed_{idx}\"\n",
    "                unparsed_data[sample_id] = {\n",
    "                    \"error_number\": item['number'],\n",
    "                    \"error_passage\": item['passage'],\n",
    "                    \"dataset\": item['dataset'],\n",
    "                    \"operation\": item['operation'],\n",
    "                    \"error_annotation\": item['error_annotation'],\n",
    "                    # 以下字段需补充（若有正确数据）\n",
    "                    \"correct_number\": \"\",  # 需手动补充或从原始数据推导\n",
    "                    \"correct_passage\": \"\"  # 需手动补充或从原始数据推导\n",
    "                }\n",
    "        \n",
    "        domain = item['dataset']\n",
    "        operation = item['operation']\n",
    "        prompt_type = item['prompt_type']\n",
    "        error_types = [k for k, v in item['error_annotation'].items() if v > 0]\n",
    "        \n",
    "        # 计算总体指标\n",
    "        if pred == expected:\n",
    "            if expected == 'yes':\n",
    "                metrics['TP'] += 1\n",
    "                for et in error_types:\n",
    "                    detailed_metrics['by_error_type'][et]['TP'] += 1\n",
    "                detailed_metrics['by_domain'][domain]['TP'] += 1\n",
    "                detailed_metrics['by_operation'][operation]['TP'] += 1\n",
    "                detailed_metrics['by_prompt_type'][prompt_type]['TP'] += 1\n",
    "            else:  # expected == 'no'\n",
    "                metrics['TN'] += 1\n",
    "                for et in error_types:\n",
    "                    detailed_metrics['by_error_type'][et]['TN'] += 1\n",
    "                detailed_metrics['by_domain'][domain]['TN'] += 1\n",
    "                detailed_metrics['by_operation'][operation]['TN'] += 1\n",
    "                detailed_metrics['by_prompt_type'][prompt_type]['TN'] += 1\n",
    "        else:\n",
    "            if expected == 'yes':\n",
    "                metrics['FN'] += 1\n",
    "                for et in error_types:\n",
    "                    detailed_metrics['by_error_type'][et]['FN'] += 1\n",
    "                detailed_metrics['by_domain'][domain]['FN'] += 1\n",
    "                detailed_metrics['by_operation'][operation]['FN'] += 1\n",
    "                detailed_metrics['by_prompt_type'][prompt_type]['FN'] += 1\n",
    "            else:  # expected == 'no'\n",
    "                metrics['FP'] += 1\n",
    "                for et in error_types:\n",
    "                    detailed_metrics['by_error_type'][et]['FP'] += 1\n",
    "                detailed_metrics['by_domain'][domain]['FP'] += 1\n",
    "                detailed_metrics['by_operation'][operation]['FP'] += 1\n",
    "                detailed_metrics['by_prompt_type'][prompt_type]['FP'] += 1\n",
    "        \n",
    "        if pred == 'generation_error':\n",
    "            metrics['Generation Error'] += 1\n",
    "            for et in error_types:\n",
    "                detailed_metrics['by_error_type'][et]['Generation Error'] += 1\n",
    "            detailed_metrics['by_domain'][domain]['Generation Error'] += 1\n",
    "            detailed_metrics['by_operation'][operation]['Generation Error'] += 1\n",
    "            detailed_metrics['by_prompt_type'][prompt_type]['Generation Error'] += 1\n",
    "    \n",
    "    # 保存无法解析的数据到 JSON\n",
    "    if unparsed_data:\n",
    "        with open(unparsed_output_file, 'w', encoding='utf-8') as f:\n",
    "            json.dump(unparsed_data, f, indent=2, ensure_ascii=False)\n",
    "        # print(f\"无法解析的 {len(unparsed_data)} 条数据已保存到 {unparsed_output_file}\")\n",
    "        print(\"注意：JSON 文件仅包含 expected_answer='Yes' 的样本，correct_number 和 correct_passage 需手动补充\")\n",
    "    else:\n",
    "        print(\"没有无法解析的数据\")\n",
    "    \n",
    "    total = len(data_list)\n",
    "    metrics['Accuracy'] = (metrics['TP'] + metrics['TN']) / total if total > 0 else 0\n",
    "    return metrics, detailed_metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "注意：JSON 文件仅包含 expected_answer='Yes' 的样本，correct_number 和 correct_passage 需手动补充\n",
      "\n",
      "Overall Metrics:\n",
      "FP: 4619 (0.481)\n",
      "TP: 4347 (0.453)\n",
      "TN: 181 (0.019)\n",
      "FN: 453 (0.047)\n",
      "Generation Error: 634 (0.066)\n",
      "Accuracy: 0.472\n",
      "\n",
      "Metrics by Domain:\n",
      "Numeracy_600K_article_title: {'FP': 994, 'TP': 997, 'TN': 6, 'FN': 3, 'Generation Error': 1}\n",
      "aclsent: {'FP': 904, 'TP': 921, 'TN': 44, 'FN': 27, 'Generation Error': 8}\n",
      "DROP: {'FP': 921, 'TP': 669, 'TN': 73, 'FN': 325, 'Generation Error': 550}\n",
      "qa-text-source-comparison: {'FP': 872, 'TP': 867, 'FN': 57, 'TN': 52, 'Generation Error': 35}\n",
      "FinNum: {'FP': 928, 'TP': 893, 'FN': 41, 'Generation Error': 40, 'TN': 6}\n",
      "\n",
      "Metrics by Error Type:\n",
      "Error in Number Relationships: {'FP': 190, 'TP': 187, 'TN': 6, 'FN': 9, 'Generation Error': 7}\n",
      "Undetectable Error: {'FP': 457, 'TP': 446, 'TN': 7, 'FN': 18, 'Generation Error': 24}\n",
      "Type Error: {'FP': 502, 'TP': 495, 'TN': 16, 'FN': 23, 'Generation Error': 23}\n",
      "Anomaly: {'FP': 216, 'TP': 214, 'TN': 14, 'FN': 16, 'Generation Error': 15}\n",
      "Improper Data: {'FP': 29, 'TP': 28, 'FN': 1}\n",
      "Factual Error: {'FP': 49, 'TP': 52, 'TN': 8, 'FN': 5, 'Generation Error': 3}\n",
      "\n",
      "Metrics by Operation:\n",
      "*2: {'FP': 155, 'TP': 143, 'TN': 4, 'FN': 16, 'Generation Error': 24}\n",
      "-10: {'FP': 178, 'TP': 162, 'FN': 21, 'TN': 5, 'Generation Error': 25}\n",
      "+1: {'FP': 177, 'TP': 166, 'TN': 11, 'FN': 22, 'Generation Error': 26}\n",
      "*0.9: {'FP': 179, 'TP': 167, 'Generation Error': 29, 'FN': 19, 'TN': 7}\n",
      "*1.1: {'FP': 190, 'TP': 180, 'FN': 19, 'TN': 9, 'Generation Error': 35}\n",
      "-0.5: {'FP': 148, 'TP': 148, 'TN': 8, 'Generation Error': 11, 'FN': 8}\n",
      "+1000: {'FP': 150, 'TP': 139, 'FN': 20, 'TN': 9, 'Generation Error': 27}\n",
      "*1.5: {'FP': 157, 'TP': 153, 'TN': 7, 'FN': 11, 'Generation Error': 15}\n",
      "*0.1: {'FP': 155, 'TP': 148, 'FN': 13, 'TN': 6, 'Generation Error': 14}\n",
      "*0: {'FP': 173, 'TP': 163, 'TN': 5, 'FN': 15, 'Generation Error': 20}\n",
      "-0.1: {'FP': 184, 'TP': 170, 'TN': 4, 'FN': 18, 'Generation Error': 30}\n",
      "swap: {'FP': 335, 'TP': 311, 'FN': 37, 'TN': 13, 'Generation Error': 45}\n",
      "*0.01: {'FP': 177, 'TP': 170, 'FN': 11, 'Generation Error': 16, 'TN': 4}\n",
      "+10: {'FP': 166, 'TP': 160, 'Generation Error': 21, 'TN': 8, 'FN': 14}\n",
      "+0.1: {'FP': 160, 'TP': 150, 'TN': 3, 'FN': 13, 'Generation Error': 24}\n",
      "*(-1): {'FP': 157, 'TP': 143, 'TN': 3, 'Generation Error': 29, 'FN': 17}\n",
      "-1: {'FP': 192, 'TP': 185, 'FN': 15, 'TN': 8, 'Generation Error': 15}\n",
      "*0.5: {'FP': 200, 'TP': 187, 'TN': 9, 'FN': 22, 'Generation Error': 24}\n",
      "*100: {'FP': 153, 'TP': 141, 'TN': 8, 'FN': 20, 'Generation Error': 29}\n",
      "+0.5: {'FP': 162, 'TP': 153, 'FN': 16, 'Generation Error': 29, 'TN': 7}\n",
      "-1000: {'FP': 161, 'TP': 144, 'TN': 3, 'FN': 20, 'Generation Error': 32}\n",
      "-100: {'FP': 148, 'TP': 136, 'TN': 7, 'FN': 19, 'Generation Error': 21}\n",
      "*0.001: {'FP': 151, 'TP': 142, 'TN': 6, 'FN': 15, 'Generation Error': 24}\n",
      "*0.7: {'FP': 124, 'TP': 122, 'TN': 5, 'FN': 7, 'Generation Error': 11}\n",
      "*1000: {'FP': 162, 'TP': 154, 'TN': 9, 'Generation Error': 25, 'FN': 17}\n",
      "+100: {'FP': 157, 'TP': 153, 'TN': 8, 'FN': 12, 'Generation Error': 12}\n",
      "*10: {'FP': 168, 'TP': 157, 'TN': 5, 'FN': 16, 'Generation Error': 21}\n",
      "\n",
      "Metrics by Prompt Type:\n",
      "few_shot: {'FP': 79, 'TP': 70, 'TN': 21, 'FN': 30, 'Generation Error': 28}\n",
      "zero_shot: {'FP': 4540, 'TP': 4277, 'TN': 160, 'FN': 423, 'Generation Error': 606}\n"
     ]
    }
   ],
   "source": [
    "# 读取 predictions.jsonl\n",
    "data_list = []\n",
    "input_file = 'xiaomi_predictions.jsonl'  # 确认路径\n",
    "with open(input_file, 'r', encoding='utf-8') as f:\n",
    "    for line in f:\n",
    "        data = json.loads(line)\n",
    "        data_list.append(data)\n",
    "\n",
    "# 评测模型并保存无法解析的数据\n",
    "unparsed_output_file = 'xiaomi_unparsed_predictions.json'\n",
    "metrics, detailed_metrics = evaluate_model(data_list, unparsed_output_file)\n",
    "\n",
    "# 打印总体指标\n",
    "print(\"\\nOverall Metrics:\")\n",
    "total = len(data_list)\n",
    "for key, value in metrics.items():\n",
    "    if key == 'Accuracy':\n",
    "        print(f\"{key}: {value:.3f}\")\n",
    "    else:\n",
    "        print(f\"{key}: {value} ({value / total:.3f})\")\n",
    "\n",
    "# 打印分维度指标\n",
    "print(\"\\nMetrics by Domain:\")\n",
    "for domain, counts in detailed_metrics['by_domain'].items():\n",
    "    print(f\"{domain}: {dict(counts)}\")\n",
    "\n",
    "print(\"\\nMetrics by Error Type:\")\n",
    "for error_type, counts in detailed_metrics['by_error_type'].items():\n",
    "    print(f\"{error_type}: {dict(counts)}\")\n",
    "\n",
    "print(\"\\nMetrics by Operation:\")\n",
    "for operation, counts in detailed_metrics['by_operation'].items():\n",
    "    print(f\"{operation}: {dict(counts)}\")\n",
    "\n",
    "print(\"\\nMetrics by Prompt Type:\")\n",
    "for prompt_type, counts in detailed_metrics['by_prompt_type'].items():\n",
    "    print(f\"{prompt_type}: {dict(counts)}\")"
   ]
  }
 ],
 "metadata": {
  "kaggle": {
   "accelerator": "gpu",
   "dataSources": [
    {
     "datasetId": 7381062,
     "sourceId": 11757485,
     "sourceType": "datasetVersion"
    },
    {
     "datasetId": 7386290,
     "sourceId": 11765571,
     "sourceType": "datasetVersion"
    }
   ],
   "dockerImageVersionId": 31011,
   "isGpuEnabled": true,
   "isInternetEnabled": true,
   "language": "python",
   "sourceType": "notebook"
  },
  "kernelspec": {
   "display_name": "temp_train",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.21"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
